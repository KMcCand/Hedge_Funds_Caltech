{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e677baf",
   "metadata": {
    "id": "9e677baf"
   },
   "source": [
    "# Problem Set 5: Neural Nets for Factor Investing\n",
    "\n",
    "California Institute of Technology\n",
    "\n",
    "BEM 114: Hedge Funds\n",
    "\n",
    "Spring 2024\n",
    "\n",
    "\n",
    "Due: May 19th, 2024\n",
    "\n",
    "Please submit your assignments on Canvas before 11:59 p.m. on the due date. Please upload both the Jupyter (iPython) notebook and a PDF of the notebook.  You do not need to submit the data.\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "In this problem set you will fit a simple feed-forward Neural Net in order to demonstrate how a neural net can be used to identify dependencies across factors in order to create a higher alpha.\n",
    "\n",
    "In question 1 we'll explore the comparison of a neural net to a linear model fit, and demonstrate some of the reasoning why a ReLU activation function is a common choice for neural nets in finance. In question 2 we'll analyze whether lag parameters may help further increase the performance of our model. In question 3 we'll investigate the importance of checking and robustness and reproducibility of the results of our fit.\n",
    "\n",
    "I have included some sample output as a sanity check for some questions. As discussed in question 3, the optimization of the neural net uses a stochastic component, and thus, your estimates will be slightly different from when you see here.\n",
    "\n",
    "## Starting Code\n",
    "You'll be asked to update the following starting code in order to update the model being fit. This will include model parameters, as well as updating the input space.\n",
    "\n",
    "The `fitting_returns_data` function is the main function that we'll run in order to pull results.\n",
    "\n",
    "The functions with an `io_` prefix define input and output variables. The functions with a `'model_` prefix define the model fit that we'll be using.\n",
    "\n",
    "This problem set will ask you to create your own `io_` and `model_` functions in order to test different modeling possibilities, and to determine the importance of these decisions in the model fitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18326965",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "18326965",
    "outputId": "c0efa0e7-a5f0-466e-d91f-57a26b1f4ad2"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We'll suppress some warnings from tensorflow, but feel free to\n",
    "# comment out this line and see some of the efficiency gains we can get!\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Note: All io_fns and model_fns start with an 'io_' and 'model_' prefix\n",
    "# respectively\n",
    "\n",
    "\n",
    "\n",
    "def fitting_returns_data(data_path,\n",
    "                         io_fn,\n",
    "                         model_fn,\n",
    "                         seed = None,\n",
    "                         print_summary = True):\n",
    "    \"\"\"\n",
    "    Fits data to supplied model and provides returns and alpha estimates.\n",
    "\n",
    "        Parameters:\n",
    "            data_path (str): path to factor data with first column as date\n",
    "                6 factors, and the last column as risk free rate\n",
    "            io_fn (fn): a function to calculate input/output pairs for the\n",
    "                model fit\n",
    "            model_fn (fn): a function that takes in input data, output data,\n",
    "                and training and testing values, and returns the strategy\n",
    "                for each day, and the observed return for each day\n",
    "            seed (int): if given, the seed for the model fit will be set to\n",
    "                this value\n",
    "            print_summary (bool): if True, then the model won't print a summary\n",
    "                of the OLS fit for our strategy return.\n",
    "        Returns:\n",
    "            strat_df (pd.DataFrame): The weights we would give each factor\n",
    "                on each day\n",
    "            return_vector (list): The returns from our strategy on each day\n",
    "            model_OLS (RegressionResultsWrapper): A regression calculating the\n",
    "                alpha for our strategy\n",
    "    \"\"\"\n",
    "    # if given, set the random seed\n",
    "    if seed:\n",
    "        set_seed(seed)\n",
    "\n",
    "    # Load the data into a pandas DataFrame\n",
    "    data = pd.read_csv(data_path)\n",
    "    # Drop date and risk free rate\n",
    "    data = data.iloc[:, 1:7]\n",
    "\n",
    "    # Shift the data by one time step to create input/output pairs\n",
    "    X, y = io_fn(data)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Fit the model\n",
    "    strat_df, return_vector = model_fn(X, y, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Calculate alpha\n",
    "    y_ols = sm.add_constant(y)\n",
    "    model_OLS = sm.OLS(return_vector, y_ols).fit()\n",
    "    if print_summary:\n",
    "        print(model_OLS.summary())\n",
    "\n",
    "    return strat_df, return_vector, model_OLS\n",
    "\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    # Adding a fixed seed from this solution: https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras\n",
    "    # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    # 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(int(seed_value))\n",
    "\n",
    "    # 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.compat.v1.set_random_seed(seed_value)\n",
    "    return\n",
    "\n",
    "def io_day_1_lag(data):\n",
    "    # Create input output pairs, where the input is the previous day of data\n",
    "    # and the output is the current day of data.\n",
    "    X = data.shift(1).dropna().reset_index(drop=True)\n",
    "    y = data.dropna().iloc[1:,:].reset_index(drop=True)\n",
    "    return X, y\n",
    "\n",
    "def io_day_1_lag_second_order_input(data):\n",
    "    # Create input output pairs where input data includes second order interactions\n",
    "    X, y = io_day_1_lag(data)\n",
    "    cols = X.columns\n",
    "\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            col_name = cols[i] + cols[j]\n",
    "            col_values = X[cols[i]] * X[cols[j]]\n",
    "            X[col_name] = col_values\n",
    "    return X, y\n",
    "\n",
    "def model_feed_forward(X, y, X_train, y_train, X_val, y_val):\n",
    "    # Define the neural network model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(6, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Train the model, verbose = 0 means reports aren't printed\n",
    "    # at the end of each epoch\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=50,\n",
    "              validation_data=(X_val, y_val))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "\n",
    "    return predictions_to_returns(pred_df, y)\n",
    "\n",
    "def predictions_to_returns(pred_df, y):\n",
    "    # Given the predictions of each factor for each day, calculate our\n",
    "    # strategy for each day, and the returns for each day\n",
    "\n",
    "    # Apply our strategy to our predictions\n",
    "    strat_df = pred_df.apply(lambda row : max_predicted_factor_strat(row), axis = 1)\n",
    "\n",
    "    # Calculate our returns\n",
    "    return_vector = np.multiply(strat_df,np.asarray(y)).apply(sum, axis = 1)\n",
    "\n",
    "    return strat_df, return_vector\n",
    "\n",
    "\n",
    "def max_predicted_factor_strat(row):\n",
    "    # For each day, set our strategy to be the factor with\n",
    "    # the highest predicted return\n",
    "    max_pred_return = max(row)\n",
    "    row_list = [x == max_pred_return for x in row]\n",
    "    return pd.Series(row_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ed3e4",
   "metadata": {
    "id": "765ed3e4"
   },
   "source": [
    "## Question 0 - Run the Neural Net and Interpet the alpha\n",
    "The following code fits a feed-forward neural net on all data, and prints a summary.\n",
    "Run the code and provide an interpretation of the alpha.\n",
    "\n",
    "```\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_1_lag,\n",
    "    model_feed_forward);\n",
    "```\n",
    "\n",
    "**Note**: The test-train split we're employing in these model fits is faulty. We use data from the future to predict our model, so all results here have some level of overfitting. However, I separately fit the models by only using past data, and we were able to get similar results, so the results still seem robust to the potential overfitting.\n",
    "\n",
    "### Question 0 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f49a61a",
   "metadata": {
    "id": "3f49a61a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 533us/step - loss: 0.4640 - val_loss: 0.4482\n",
      "Epoch 2/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 0.4371 - val_loss: 0.4460\n",
      "Epoch 3/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 0.4344 - val_loss: 0.4450\n",
      "Epoch 4/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - loss: 0.4323 - val_loss: 0.4443\n",
      "Epoch 5/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - loss: 0.4308 - val_loss: 0.4441\n",
      "Epoch 6/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 0.4297 - val_loss: 0.4437\n",
      "Epoch 7/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 0.4289 - val_loss: 0.4438\n",
      "Epoch 8/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 0.4281 - val_loss: 0.4439\n",
      "Epoch 9/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398us/step - loss: 0.4274 - val_loss: 0.4441\n",
      "Epoch 10/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422us/step - loss: 0.4268 - val_loss: 0.4441\n",
      "Epoch 11/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397us/step - loss: 0.4262 - val_loss: 0.4442\n",
      "Epoch 12/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 0.4254 - val_loss: 0.4443\n",
      "Epoch 13/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384us/step - loss: 0.4249 - val_loss: 0.4444\n",
      "Epoch 14/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 0.4245 - val_loss: 0.4443\n",
      "Epoch 15/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 0.4240 - val_loss: 0.4443\n",
      "Epoch 16/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - loss: 0.4235 - val_loss: 0.4447\n",
      "Epoch 17/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384us/step - loss: 0.4230 - val_loss: 0.4448\n",
      "Epoch 18/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404us/step - loss: 0.4226 - val_loss: 0.4448\n",
      "Epoch 19/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - loss: 0.4222 - val_loss: 0.4449\n",
      "Epoch 20/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - loss: 0.4219 - val_loss: 0.4452\n",
      "Epoch 21/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - loss: 0.4213 - val_loss: 0.4456\n",
      "Epoch 22/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.4209 - val_loss: 0.4456\n",
      "Epoch 23/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384us/step - loss: 0.4206 - val_loss: 0.4455\n",
      "Epoch 24/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409us/step - loss: 0.4201 - val_loss: 0.4459\n",
      "Epoch 25/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - loss: 0.4198 - val_loss: 0.4460\n",
      "Epoch 26/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - loss: 0.4195 - val_loss: 0.4461\n",
      "Epoch 27/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.4192 - val_loss: 0.4462\n",
      "Epoch 28/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 0.4189 - val_loss: 0.4462\n",
      "Epoch 29/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407us/step - loss: 0.4185 - val_loss: 0.4466\n",
      "Epoch 30/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - loss: 0.4183 - val_loss: 0.4469\n",
      "Epoch 31/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396us/step - loss: 0.4181 - val_loss: 0.4468\n",
      "Epoch 32/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step - loss: 0.4178 - val_loss: 0.4469\n",
      "Epoch 33/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415us/step - loss: 0.4174 - val_loss: 0.4471\n",
      "Epoch 34/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.4169 - val_loss: 0.4473\n",
      "Epoch 35/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419us/step - loss: 0.4168 - val_loss: 0.4475\n",
      "Epoch 36/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420us/step - loss: 0.4165 - val_loss: 0.4476\n",
      "Epoch 37/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442us/step - loss: 0.4161 - val_loss: 0.4475\n",
      "Epoch 38/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400us/step - loss: 0.4159 - val_loss: 0.4477\n",
      "Epoch 39/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 0.4156 - val_loss: 0.4482\n",
      "Epoch 40/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403us/step - loss: 0.4154 - val_loss: 0.4485\n",
      "Epoch 41/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - loss: 0.4151 - val_loss: 0.4481\n",
      "Epoch 42/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367us/step - loss: 0.4149 - val_loss: 0.4485\n",
      "Epoch 43/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - loss: 0.4146 - val_loss: 0.4485\n",
      "Epoch 44/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 0.4144 - val_loss: 0.4484\n",
      "Epoch 45/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - loss: 0.4141 - val_loss: 0.4492\n",
      "Epoch 46/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step - loss: 0.4140 - val_loss: 0.4488\n",
      "Epoch 47/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408us/step - loss: 0.4138 - val_loss: 0.4492\n",
      "Epoch 48/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385us/step - loss: 0.4138 - val_loss: 0.4498\n",
      "Epoch 49/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406us/step - loss: 0.4134 - val_loss: 0.4498\n",
      "Epoch 50/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - loss: 0.4133 - val_loss: 0.4502\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.134\n",
      "Model:                            OLS   Adj. R-squared:                  0.133\n",
      "Method:                 Least Squares   F-statistic:                     385.1\n",
      "Date:                Sat, 18 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        20:28:30   Log-Likelihood:                -16108.\n",
      "No. Observations:               14998   AIC:                         3.223e+04\n",
      "Df Residuals:                   14991   BIC:                         3.228e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1313      0.006     22.620      0.000       0.120       0.143\n",
      "Mkt-RF         0.2234      0.006     35.924      0.000       0.211       0.236\n",
      "SMB            0.0576      0.011      5.083      0.000       0.035       0.080\n",
      "HML            0.2291      0.013     17.566      0.000       0.204       0.255\n",
      "RMW            0.1367      0.016      8.758      0.000       0.106       0.167\n",
      "CMA            0.0070      0.020      0.348      0.728      -0.032       0.046\n",
      "MOM            0.2748      0.008     33.882      0.000       0.259       0.291\n",
      "==============================================================================\n",
      "Omnibus:                     4283.507   Durbin-Watson:                   1.957\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           270644.962\n",
      "Skew:                           0.505   Prob(JB):                         0.00\n",
      "Kurtosis:                      23.786   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "COMMON_SEED = 519\n",
    "\n",
    "# Calculate returns and alphas using feed forward neural net model\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_1_lag,\n",
    "    model_feed_forward,\n",
    "    seed = COMMON_SEED);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d6dd8",
   "metadata": {
    "id": "d26d6dd8"
   },
   "source": [
    "The alpha value of 0.1232 is positive and significant at the 1% level, indicating that using Neural Networks to identify relationships between the six factors is a profitable hedge fund strategy.\n",
    "\n",
    "## Question 1 - Comparing Neural Net to OLS\n",
    "\n",
    "### 1ai [20 points] Set a linear activation function\n",
    "For the feed-forward neural net that is fit above, the `model_feed_forward` function sets up activation across hidden layers with the following code:\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(6,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(6, activation='linear'))\n",
    "```\n",
    "\n",
    "Write a new function called `model_feed_forward_linear`, which changes the `'relu'` parameter to `'linear'`.\n",
    "\n",
    "\n",
    "Calculate reults for the neural net with linear activation. How does this compare to the ReLU activation from Question 0?\n",
    "\n",
    "### Solution 1ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1106235",
   "metadata": {
    "id": "c1106235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 523us/step - loss: 0.4971 - val_loss: 0.4450\n",
      "Epoch 2/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.4401 - val_loss: 0.4440\n",
      "Epoch 3/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - loss: 0.4392 - val_loss: 0.4437\n",
      "Epoch 4/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384us/step - loss: 0.4389 - val_loss: 0.4435\n",
      "Epoch 5/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385us/step - loss: 0.4387 - val_loss: 0.4434\n",
      "Epoch 6/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384us/step - loss: 0.4386 - val_loss: 0.4433\n",
      "Epoch 7/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - loss: 0.4384 - val_loss: 0.4433\n",
      "Epoch 8/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.4383 - val_loss: 0.4432\n",
      "Epoch 9/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - loss: 0.4381 - val_loss: 0.4432\n",
      "Epoch 10/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384us/step - loss: 0.4380 - val_loss: 0.4432\n",
      "Epoch 11/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386us/step - loss: 0.4379 - val_loss: 0.4432\n",
      "Epoch 12/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - loss: 0.4378 - val_loss: 0.4431\n",
      "Epoch 13/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - loss: 0.4377 - val_loss: 0.4431\n",
      "Epoch 14/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392us/step - loss: 0.4376 - val_loss: 0.4431\n",
      "Epoch 15/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409us/step - loss: 0.4375 - val_loss: 0.4431\n",
      "Epoch 16/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.4374 - val_loss: 0.4431\n",
      "Epoch 17/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - loss: 0.4373 - val_loss: 0.4431\n",
      "Epoch 18/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 0.4372 - val_loss: 0.4431\n",
      "Epoch 19/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.4371 - val_loss: 0.4431\n",
      "Epoch 20/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - loss: 0.4370 - val_loss: 0.4431\n",
      "Epoch 21/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - loss: 0.4369 - val_loss: 0.4431\n",
      "Epoch 22/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405us/step - loss: 0.4369 - val_loss: 0.4431\n",
      "Epoch 23/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - loss: 0.4368 - val_loss: 0.4431\n",
      "Epoch 24/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step - loss: 0.4367 - val_loss: 0.4431\n",
      "Epoch 25/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 0.4367 - val_loss: 0.4431\n",
      "Epoch 26/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 0.4366 - val_loss: 0.4431\n",
      "Epoch 27/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - loss: 0.4366 - val_loss: 0.4431\n",
      "Epoch 28/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - loss: 0.4365 - val_loss: 0.4431\n",
      "Epoch 29/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step - loss: 0.4364 - val_loss: 0.4431\n",
      "Epoch 30/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - loss: 0.4364 - val_loss: 0.4431\n",
      "Epoch 31/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385us/step - loss: 0.4364 - val_loss: 0.4432\n",
      "Epoch 32/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step - loss: 0.4363 - val_loss: 0.4432\n",
      "Epoch 33/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404us/step - loss: 0.4363 - val_loss: 0.4432\n",
      "Epoch 34/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step - loss: 0.4362 - val_loss: 0.4432\n",
      "Epoch 35/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - loss: 0.4362 - val_loss: 0.4432\n",
      "Epoch 36/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414us/step - loss: 0.4362 - val_loss: 0.4432\n",
      "Epoch 37/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - loss: 0.4361 - val_loss: 0.4432\n",
      "Epoch 38/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step - loss: 0.4361 - val_loss: 0.4432\n",
      "Epoch 39/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386us/step - loss: 0.4361 - val_loss: 0.4432\n",
      "Epoch 40/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411us/step - loss: 0.4360 - val_loss: 0.4432\n",
      "Epoch 41/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.4360 - val_loss: 0.4432\n",
      "Epoch 42/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - loss: 0.4360 - val_loss: 0.4432\n",
      "Epoch 43/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404us/step - loss: 0.4360 - val_loss: 0.4432\n",
      "Epoch 44/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.4359 - val_loss: 0.4432\n",
      "Epoch 45/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 0.4359 - val_loss: 0.4432\n",
      "Epoch 46/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - loss: 0.4359 - val_loss: 0.4432\n",
      "Epoch 47/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403us/step - loss: 0.4359 - val_loss: 0.4433\n",
      "Epoch 48/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - loss: 0.4359 - val_loss: 0.4433\n",
      "Epoch 49/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - loss: 0.4358 - val_loss: 0.4433\n",
      "Epoch 50/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400us/step - loss: 0.4358 - val_loss: 0.4433\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.134\n",
      "Model:                            OLS   Adj. R-squared:                  0.134\n",
      "Method:                 Least Squares   F-statistic:                     388.2\n",
      "Date:                Sat, 18 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        20:28:39   Log-Likelihood:                -15414.\n",
      "No. Observations:               14998   AIC:                         3.084e+04\n",
      "Df Residuals:                   14991   BIC:                         3.090e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0874      0.006     15.765      0.000       0.077       0.098\n",
      "Mkt-RF         0.1491      0.006     25.115      0.000       0.138       0.161\n",
      "SMB            0.1603      0.011     14.826      0.000       0.139       0.182\n",
      "HML            0.1811      0.012     14.544      0.000       0.157       0.205\n",
      "RMW            0.1454      0.015      9.761      0.000       0.116       0.175\n",
      "CMA            0.1670      0.019      8.728      0.000       0.130       0.205\n",
      "MOM            0.2921      0.008     37.717      0.000       0.277       0.307\n",
      "==============================================================================\n",
      "Omnibus:                     3883.732   Durbin-Watson:                   2.000\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           235095.780\n",
      "Skew:                          -0.341   Prob(JB):                         0.00\n",
      "Kurtosis:                      22.384   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           0      1      2      3      4      5\n",
       " 0       True  False  False  False  False  False\n",
       " 1      False  False  False  False  False   True\n",
       " 2      False  False  False  False  False   True\n",
       " 3      False   True  False  False  False  False\n",
       " 4       True  False  False  False  False  False\n",
       " ...      ...    ...    ...    ...    ...    ...\n",
       " 14993  False  False  False  False  False   True\n",
       " 14994  False  False   True  False  False  False\n",
       " 14995   True  False  False  False  False  False\n",
       " 14996   True  False  False  False  False  False\n",
       " 14997  False  False  False  False  False   True\n",
       " \n",
       " [14998 rows x 6 columns],\n",
       " 0        0.79\n",
       " 1        0.41\n",
       " 2        0.07\n",
       " 3        0.07\n",
       " 4        0.45\n",
       "          ... \n",
       " 14993    0.14\n",
       " 14994    0.01\n",
       " 14995    0.36\n",
       " 14996   -1.38\n",
       " 14997   -0.70\n",
       " Length: 14998, dtype: float64,\n",
       " <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x2a33ab610>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define linear feed-forward neural net based on model_feed_forward\n",
    "def model_feed_forward_linear(X, y, X_train, y_train, X_val, y_val):\n",
    "    # Define the neural network model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='linear', input_shape=(X.shape[1],)))\n",
    "    model.add(Dense(16, activation='linear'))\n",
    "    model.add(Dense(6, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Train the model, verbose = 0 means reports aren't printed\n",
    "    # at the end of each epoch\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=50,\n",
    "              validation_data=(X_val, y_val))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "\n",
    "    return predictions_to_returns(pred_df, y)\n",
    "\n",
    "# Calculate returns and alphas from linear neural net\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_1_lag,\n",
    "    model_feed_forward_linear,\n",
    "    seed = COMMON_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XzogcR_otKvx",
   "metadata": {
    "id": "XzogcR_otKvx"
   },
   "source": [
    "The ReLU alpha of 0.1232 is higher than the linear alpha of 0.0950, while both are statistically significant at the 1% level. This indicates that the ReLU activation function better supports neural networks to identify profitable relationships between factors, though we may need more in-depth experiments to conclude this convincingly.\n",
    "\n",
    "### 1aii [10 points] Compare with linear model\n",
    "\n",
    "The following code defines a function called `model_linear_fit`, which fits a linear model on y_train and X_train and outputs the predicted return given `X`, called `pred_df`. The function then calculates the  returns of the linear model by running `predictions_to_returns(pred_df, y)`.\n",
    "\n",
    "Since the neural net is only fitting on linear relationships, we should see similar results across the linear neural net and the linear OLS model here.\n",
    "\n",
    "Calculate reults for the linear model below. How does the alpha compare to the linear neural net?  \n",
    "\n",
    "### Solution 1aii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d97083",
   "metadata": {
    "id": "09d97083",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.127\n",
      "Model:                            OLS   Adj. R-squared:                  0.127\n",
      "Method:                 Least Squares   F-statistic:                     363.2\n",
      "Date:                Sat, 18 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        20:28:40   Log-Likelihood:                -15513.\n",
      "No. Observations:               14998   AIC:                         3.104e+04\n",
      "Df Residuals:                   14991   BIC:                         3.109e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0904      0.006     16.202      0.000       0.079       0.101\n",
      "Mkt-RF         0.1371      0.006     22.937      0.000       0.125       0.149\n",
      "SMB            0.1647      0.011     15.134      0.000       0.143       0.186\n",
      "HML            0.2111      0.013     16.842      0.000       0.187       0.236\n",
      "RMW            0.1475      0.015      9.833      0.000       0.118       0.177\n",
      "CMA            0.1534      0.019      7.967      0.000       0.116       0.191\n",
      "MOM            0.2758      0.008     35.382      0.000       0.261       0.291\n",
      "==============================================================================\n",
      "Omnibus:                     3960.117   Durbin-Watson:                   2.005\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           262916.099\n",
      "Skew:                          -0.336   Prob(JB):                         0.00\n",
      "Kurtosis:                      23.501   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           0      1      2      3      4      5\n",
       " 0       True  False  False  False  False  False\n",
       " 1      False  False  False  False  False   True\n",
       " 2      False  False  False  False  False   True\n",
       " 3      False   True  False  False  False  False\n",
       " 4      False  False  False  False   True  False\n",
       " ...      ...    ...    ...    ...    ...    ...\n",
       " 14993  False  False  False  False  False   True\n",
       " 14994  False  False   True  False  False  False\n",
       " 14995   True  False  False  False  False  False\n",
       " 14996   True  False  False  False  False  False\n",
       " 14997  False  False  False  False  False   True\n",
       " \n",
       " [14998 rows x 6 columns],\n",
       " 0        0.79\n",
       " 1        0.41\n",
       " 2        0.07\n",
       " 3        0.07\n",
       " 4       -0.01\n",
       "          ... \n",
       " 14993    0.14\n",
       " 14994    0.01\n",
       " 14995    0.36\n",
       " 14996   -1.38\n",
       " 14997   -0.70\n",
       " Length: 14998, dtype: float64,\n",
       " <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x2a394ed10>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define linear OLS model\n",
    "## Linear Model Fit\n",
    "def model_linear_fit(X, y, X_train, y_train, X_val, y_val):\n",
    "    model_OLS = sm.OLS(y_train, X_train).fit()\n",
    "    # Make predictions\n",
    "    predictions = model_OLS.predict(X)\n",
    "\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "\n",
    "    return predictions_to_returns(pred_df, y)\n",
    "\n",
    "# Calculate returns and alphas using linear OLS model\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_1_lag,\n",
    "    model_linear_fit,\n",
    "    seed = COMMON_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c7ff9",
   "metadata": {
    "id": "746c7ff9"
   },
   "source": [
    "The linear model alpha of 0.0904 is very similar the neural network model alpha of 0.0950, which is as expected. Both are statistically significant at the 1% level.\n",
    "\n",
    "### 1b [20 points] Include Interaction Terms\n",
    "Linear models don't account for any interaction effects. In order to account for an interaction we can add input variables that give the product of factor returns on each day. This would be analogous to adding a interaction term to a linear model.\n",
    "\n",
    "Using the io_ function `io_day_1_lag_second_order_input`, and your `model_linear_fit` function, calculate returns while including second order inputs.\n",
    "\n",
    "Write a new io_ function `io_day_lag_third_order_input`, to also include third order fits in your input data.\n",
    "\n",
    "How do these models compare to the ReLU alpha?\n",
    "\n",
    "\n",
    "### Solution 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2921ed",
   "metadata": {
    "id": "4e2921ed",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.164\n",
      "Model:                            OLS   Adj. R-squared:                  0.163\n",
      "Method:                 Least Squares   F-statistic:                     489.2\n",
      "Date:                Sat, 18 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        20:28:41   Log-Likelihood:                -16116.\n",
      "No. Observations:               14998   AIC:                         3.225e+04\n",
      "Df Residuals:                   14991   BIC:                         3.230e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0965      0.006     16.615      0.000       0.085       0.108\n",
      "Mkt-RF         0.2727      0.006     43.824      0.000       0.261       0.285\n",
      "SMB            0.1154      0.011     10.188      0.000       0.093       0.138\n",
      "HML            0.2557      0.013     19.601      0.000       0.230       0.281\n",
      "RMW            0.1325      0.016      8.485      0.000       0.102       0.163\n",
      "CMA            0.1257      0.020      6.270      0.000       0.086       0.165\n",
      "MOM            0.2588      0.008     31.892      0.000       0.243       0.275\n",
      "==============================================================================\n",
      "Omnibus:                     3804.967   Durbin-Watson:                   2.039\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           304893.099\n",
      "Skew:                          -0.010   Prob(JB):                         0.00\n",
      "Kurtosis:                      25.088   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           0      1      2      3      4      5\n",
       " 0       True  False  False  False  False  False\n",
       " 1      False  False  False  False  False   True\n",
       " 2      False  False  False  False  False   True\n",
       " 3       True  False  False  False  False  False\n",
       " 4      False  False  False  False   True  False\n",
       " ...      ...    ...    ...    ...    ...    ...\n",
       " 14993  False  False  False  False  False   True\n",
       " 14994  False  False   True  False  False  False\n",
       " 14995   True  False  False  False  False  False\n",
       " 14996   True  False  False  False  False  False\n",
       " 14997  False  False  False  False  False   True\n",
       " \n",
       " [14998 rows x 6 columns],\n",
       " 0        0.79\n",
       " 1        0.41\n",
       " 2        0.07\n",
       " 3       -0.63\n",
       " 4       -0.01\n",
       "          ... \n",
       " 14993    0.14\n",
       " 14994    0.01\n",
       " 14995    0.36\n",
       " 14996   -1.38\n",
       " 14997   -0.70\n",
       " Length: 14998, dtype: float64,\n",
       " <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x293203d10>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate returns and alphas using linear OLS model\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_1_lag_second_order_input,\n",
    "    model_linear_fit,\n",
    "    seed = COMMON_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd49b62a",
   "metadata": {
    "id": "cd49b62a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.120\n",
      "Model:                            OLS   Adj. R-squared:                  0.120\n",
      "Method:                 Least Squares   F-statistic:                     340.7\n",
      "Date:                Sat, 18 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        20:28:42   Log-Likelihood:                -16045.\n",
      "No. Observations:               14998   AIC:                         3.210e+04\n",
      "Df Residuals:                   14991   BIC:                         3.216e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1240      0.006     21.447      0.000       0.113       0.135\n",
      "Mkt-RF         0.1922      0.006     31.031      0.000       0.180       0.204\n",
      "SMB            0.0399      0.011      3.536      0.000       0.018       0.062\n",
      "HML            0.1763      0.013     13.579      0.000       0.151       0.202\n",
      "RMW            0.1619      0.016     10.419      0.000       0.131       0.192\n",
      "CMA            0.0831      0.020      4.165      0.000       0.044       0.122\n",
      "MOM            0.2760      0.008     34.172      0.000       0.260       0.292\n",
      "==============================================================================\n",
      "Omnibus:                     4135.930   Durbin-Watson:                   1.884\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           268573.934\n",
      "Skew:                           0.431   Prob(JB):                         0.00\n",
      "Kurtosis:                      23.713   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           0      1      2      3      4      5\n",
       " 0      False  False  False  False   True  False\n",
       " 1      False  False  False  False  False   True\n",
       " 2      False  False  False  False  False   True\n",
       " 3       True  False  False  False  False  False\n",
       " 4      False  False  False  False   True  False\n",
       " ...      ...    ...    ...    ...    ...    ...\n",
       " 14993  False  False  False  False  False   True\n",
       " 14994  False  False   True  False  False  False\n",
       " 14995  False   True  False  False  False  False\n",
       " 14996  False   True  False  False  False  False\n",
       " 14997  False  False  False  False  False   True\n",
       " \n",
       " [14998 rows x 6 columns],\n",
       " 0       -0.21\n",
       " 1        0.41\n",
       " 2        0.07\n",
       " 3       -0.63\n",
       " 4       -0.01\n",
       "          ... \n",
       " 14993    0.14\n",
       " 14994    0.01\n",
       " 14995    0.32\n",
       " 14996    0.05\n",
       " 14997   -0.70\n",
       " Length: 14998, dtype: float64,\n",
       " <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x2a33b7810>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create input output pairs where input data includes second order interactions\n",
    "\n",
    "# This is actually third order as described in the question. The outputs are not\n",
    "# similar to the sanity checks that professor Sinclair wrote for us.\n",
    "# def io_day_lag_third_order_input(data):\n",
    "#     X, y = io_day_1_lag(data)\n",
    "#     cols = X.columns\n",
    "\n",
    "#     X, y = io_day_1_lag_second_order_input(data)\n",
    "\n",
    "#     for i in range(len(cols)):\n",
    "#         for j in range(i+1, len(cols)):\n",
    "#           for k in range(j+1, len(cols)):\n",
    "#             col_name = cols[i] + cols[j] + cols[k]\n",
    "#             col_values = X[cols[i]] * X[cols[j]] * X[cols[k]]\n",
    "#             X[col_name] = col_values\n",
    "\n",
    "#     return X, y\n",
    "\n",
    "# This is fourth order. The outputs agree very closely with professor Sinclair's\n",
    "# sanity checks.\n",
    "def io_day_lag_third_order_input(data):\n",
    "    # Create input output pairs where input data includes second order interactions\n",
    "    X, y = io_day_1_lag_second_order_input(data)\n",
    "    cols = X.columns\n",
    "\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            col_name = cols[i] + cols[j]\n",
    "            col_values = X[cols[i]] * X[cols[j]]\n",
    "            X[col_name] = col_values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Calculate returns and alphas using third order interactions\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_lag_third_order_input,\n",
    "    model_linear_fit,\n",
    "    seed = COMMON_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c46887",
   "metadata": {
    "id": "b1c46887"
   },
   "source": [
    "The second order linear model has an alpha of 0.0965, which is lower than the ReLU alpha of 0.1232, but is significant at the 1% level. On the other hand, the third order linear model that we implemented as fourth order to be similar to the desired output has alpha of 0.1240, which is very similar to the ReLU alpha. It is also statistically significant at the 1% level.\n",
    "\n",
    "## Question 2 - Adding Time Lag Parameters\n",
    "### 2 [25 points] Lag Time Parameters\n",
    "Our current neural net only uses the past 1 day of data. For time series data, including more lag days can be useful. For example, if a factor return is high for 2 consecutive days, that may be more informative than just knowing that the return was only high for the previous day.\n",
    "\n",
    "In this question, we will simply add a new column to our input data for each lagged data. This is analogous to an AutoRegressive Model, which is a popular financial engineering tool, [see this textbook](https://link.springer.com/book/10.1007/978-1-4939-2614-5). In the Neural Net literature, a Recurrent Neural Net is a common tool for more directly accounting for time lagged data directly in the neural net architecture, but the lagged model gets us a good amount of the way there!\n",
    "\n",
    "Starting from the `io_day_1_lag` function write an `io_day_5_lag` function, which adds to the input dataframe 5 days of lagged data per factor. This will mean your input data will now have 6*5 = 30 columns instead of 6 columns.\n",
    "\n",
    "When complete, run the following and compare results to the original ReLU model from Question 0. How do the results compare? Should we continue to to pursue incorporating lag effects in our analysis of this data?\n",
    "\n",
    "```\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_5_lag,\n",
    "    model_feed_forward);\n",
    " ```\n",
    "\n",
    " ### Question 2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ab4fd43",
   "metadata": {
    "id": "3ab4fd43",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482us/step - loss: 0.4917 - val_loss: 0.4522\n",
      "Epoch 2/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369us/step - loss: 0.4245 - val_loss: 0.4492\n",
      "Epoch 3/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.4195 - val_loss: 0.4482\n",
      "Epoch 4/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 0.4163 - val_loss: 0.4483\n",
      "Epoch 5/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 0.4129 - val_loss: 0.4478\n",
      "Epoch 6/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 0.4098 - val_loss: 0.4492\n",
      "Epoch 7/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 0.4060 - val_loss: 0.4501\n",
      "Epoch 8/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - loss: 0.4027 - val_loss: 0.4513\n",
      "Epoch 9/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.3999 - val_loss: 0.4530\n",
      "Epoch 10/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 0.3971 - val_loss: 0.4538\n",
      "Epoch 11/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.3942 - val_loss: 0.4551\n",
      "Epoch 12/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - loss: 0.3917 - val_loss: 0.4565\n",
      "Epoch 13/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - loss: 0.3892 - val_loss: 0.4588\n",
      "Epoch 14/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - loss: 0.3871 - val_loss: 0.4600\n",
      "Epoch 15/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 0.3850 - val_loss: 0.4609\n",
      "Epoch 16/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - loss: 0.3826 - val_loss: 0.4627\n",
      "Epoch 17/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - loss: 0.3808 - val_loss: 0.4646\n",
      "Epoch 18/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - loss: 0.3792 - val_loss: 0.4652\n",
      "Epoch 19/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - loss: 0.3778 - val_loss: 0.4669\n",
      "Epoch 20/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - loss: 0.3761 - val_loss: 0.4677\n",
      "Epoch 21/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step - loss: 0.3744 - val_loss: 0.4690\n",
      "Epoch 22/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 0.3726 - val_loss: 0.4716\n",
      "Epoch 23/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.3717 - val_loss: 0.4700\n",
      "Epoch 24/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - loss: 0.3698 - val_loss: 0.4744\n",
      "Epoch 25/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 0.3686 - val_loss: 0.4748\n",
      "Epoch 26/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.3676 - val_loss: 0.4768\n",
      "Epoch 27/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 0.3664 - val_loss: 0.4767\n",
      "Epoch 28/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 0.3653 - val_loss: 0.4781\n",
      "Epoch 29/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 0.3640 - val_loss: 0.4780\n",
      "Epoch 30/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.3625 - val_loss: 0.4807\n",
      "Epoch 31/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 0.3614 - val_loss: 0.4804\n",
      "Epoch 32/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - loss: 0.3600 - val_loss: 0.4824\n",
      "Epoch 33/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - loss: 0.3593 - val_loss: 0.4828\n",
      "Epoch 34/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 0.3573 - val_loss: 0.4851\n",
      "Epoch 35/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 0.3566 - val_loss: 0.4843\n",
      "Epoch 36/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.3553 - val_loss: 0.4855\n",
      "Epoch 37/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 0.3545 - val_loss: 0.4870\n",
      "Epoch 38/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - loss: 0.3538 - val_loss: 0.4863\n",
      "Epoch 39/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - loss: 0.3523 - val_loss: 0.4867\n",
      "Epoch 40/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367us/step - loss: 0.3510 - val_loss: 0.4868\n",
      "Epoch 41/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.3507 - val_loss: 0.4876\n",
      "Epoch 42/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - loss: 0.3488 - val_loss: 0.4901\n",
      "Epoch 43/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395us/step - loss: 0.3480 - val_loss: 0.4888\n",
      "Epoch 44/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - loss: 0.3474 - val_loss: 0.4903\n",
      "Epoch 45/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357us/step - loss: 0.3471 - val_loss: 0.4919\n",
      "Epoch 46/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - loss: 0.3456 - val_loss: 0.4917\n",
      "Epoch 47/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 0.3450 - val_loss: 0.4902\n",
      "Epoch 48/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.3444 - val_loss: 0.4933\n",
      "Epoch 49/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 0.3438 - val_loss: 0.4915\n",
      "Epoch 50/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - loss: 0.3433 - val_loss: 0.4942\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223us/step\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.289\n",
      "Model:                            OLS   Adj. R-squared:                  0.289\n",
      "Method:                 Least Squares   F-statistic:                     1016.\n",
      "Date:                Sat, 18 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        20:28:50   Log-Likelihood:                -16390.\n",
      "No. Observations:               14994   AIC:                         3.279e+04\n",
      "Df Residuals:                   14987   BIC:                         3.285e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1461      0.006     24.692      0.000       0.135       0.158\n",
      "Mkt-RF         0.4679      0.006     73.794      0.000       0.455       0.480\n",
      "SMB           -0.0280      0.012     -2.427      0.015      -0.051      -0.005\n",
      "HML            0.1746      0.013     13.136      0.000       0.149       0.201\n",
      "RMW            0.1129      0.016      7.101      0.000       0.082       0.144\n",
      "CMA            0.0568      0.020      2.781      0.005       0.017       0.097\n",
      "MOM            0.1896      0.008     22.938      0.000       0.173       0.206\n",
      "==============================================================================\n",
      "Omnibus:                     6359.092   Durbin-Watson:                   1.739\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           119771.381\n",
      "Skew:                           1.566   Prob(JB):                         0.00\n",
      "Kurtosis:                      16.487   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           0      1      2      3      4      5\n",
       " 0       True  False  False  False  False  False\n",
       " 1       True  False  False  False  False  False\n",
       " 2       True  False  False  False  False  False\n",
       " 3       True  False  False  False  False  False\n",
       " 4       True  False  False  False  False  False\n",
       " ...      ...    ...    ...    ...    ...    ...\n",
       " 14989  False  False   True  False  False  False\n",
       " 14990  False  False  False  False  False   True\n",
       " 14991   True  False  False  False  False  False\n",
       " 14992   True  False  False  False  False  False\n",
       " 14993  False  False   True  False  False  False\n",
       " \n",
       " [14994 rows x 6 columns],\n",
       " 0        0.45\n",
       " 1       -0.18\n",
       " 2       -0.16\n",
       " 3       -0.12\n",
       " 4       -0.62\n",
       "          ... \n",
       " 14989    0.65\n",
       " 14990   -1.23\n",
       " 14991    0.36\n",
       " 14992   -1.38\n",
       " 14993   -0.06\n",
       " Length: 14994, dtype: float64,\n",
       " <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x2a95779d0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def io_day_5_lag(data):\n",
    "    X = data.shift(1).add_suffix('_lag1')\n",
    "    for i in range(2, 6):\n",
    "        shifted_data = data.shift(i).add_suffix('_lag{}'.format(i))\n",
    "        X = pd.concat([X, shifted_data], axis=1)\n",
    "    X = X.dropna().reset_index(drop=True)\n",
    "    y = data.iloc[5:,:].reset_index(drop=True)\n",
    "    return X, y\n",
    "\n",
    "# Calculate returns and alphas using the feed forward neural net with\n",
    "# five day lagged input variables.\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_5_lag,\n",
    "    model_feed_forward,\n",
    "    seed = COMMON_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84cfba",
   "metadata": {
    "id": "3c84cfba"
   },
   "source": [
    "The 5 day lag feed-forward neural network model has an alpha of 0.1574, which is larger than the ReLU model alpha of 0.1232 and statistically significant at the 1% level. This indicates that we should continue to incorporate lag effects into our models, and supports the intuitive notion that giving the model factor information from days earlier than the previous day will improve the performance.\n",
    "\n",
    "## Question 3 - Investigating Potentials for P-Hacking\n",
    "### 3 [25 points] Randomness in Alphas\n",
    "\n",
    "Neural  nets are fit via a Stochastic Gradient Descent. This implies that there is inherent randomness in any fit of the model. One good way to account for this noise in your model is to refit the model multiple times and observe the distribution. It's more accurate to report the median or mean of these estimates, although it can be hard to tell if a paper/report has cherry-picked the best result in this way.\n",
    "\n",
    "Using the `io_day_1_lag`, and `model_feed_forward` settings, rerun the model 100 times and get a distribution for the alpha given. (You can use the `seed` parameter in the `fitting_returns_data` function if you want to be able to reproduce a given high return.)\n",
    "\n",
    "Set `print_summary = False` in `fitting_returns_data` in order to avoid large amounts of output.\n",
    "\n",
    "What's the highest return you could get if you were to ignore the importance of the robustness of a model result? What would be a downside of reporting a result like this?\n",
    "\n",
    "Note: running the model 100 times may take awhile (over an hour on Google Colab). Debug your code before attempting the 100 cycles.\n",
    "\n",
    "### Question 3 Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7000f3c5",
   "metadata": {
    "id": "7000f3c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The only supported seed types are: None,\nint, float, str, bytes, and bytearray.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m seeds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m9999\u001b[39m, size\u001b[38;5;241m=\u001b[39mnum_reruns)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_reruns):\n\u001b[0;32m----> 9\u001b[0m     model_fit \u001b[38;5;241m=\u001b[39m fitting_returns_data(\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff6_factors_19630701_20230131.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m         io_day_1_lag,\n\u001b[1;32m     12\u001b[0m         model_feed_forward,\n\u001b[1;32m     13\u001b[0m         seed \u001b[38;5;241m=\u001b[39m seeds[i],\n\u001b[1;32m     14\u001b[0m         print_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     15\u001b[0m     alphas\u001b[38;5;241m.\u001b[39mappend(model_fit\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[12], line 56\u001b[0m, in \u001b[0;36mfitting_returns_data\u001b[0;34m(data_path, io_fn, model_fn, seed, print_summary)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# if given, set the random seed\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed:\n\u001b[0;32m---> 56\u001b[0m     set_seed(seed)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Load the data into a pandas DataFrame\u001b[39;00m\n\u001b[1;32m     59\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_path)\n",
      "Cell \u001b[0;32mIn[12], line 88\u001b[0m, in \u001b[0;36mset_seed\u001b[0;34m(seed_value)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# 2. Set the `python` built-in pseudo-random generator at a fixed value\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(seed_value))\n\u001b[0;32m---> 88\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed_value)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# 3. Set the `numpy` pseudo-random generator at a fixed value\u001b[39;00m\n\u001b[1;32m     91\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed_value)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/random.py:160\u001b[0m, in \u001b[0;36mRandom.seed\u001b[0;34m(self, a, version)\u001b[0m\n\u001b[1;32m    157\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_bytes(a \u001b[38;5;241m+\u001b[39m _sha512(a)\u001b[38;5;241m.\u001b[39mdigest())\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, (\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe only supported seed types are: None,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    161\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint, float, str, bytes, and bytearray.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mseed(a)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgauss_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: The only supported seed types are: None,\nint, float, str, bytes, and bytearray."
     ]
    }
   ],
   "source": [
    "# Rerun model 100 times\n",
    "num_reruns = 100\n",
    "alphas = []\n",
    "\n",
    "np.random.seed(519)\n",
    "seeds = np.random.randint(9999, size=num_reruns)\n",
    "\n",
    "for i in range(num_reruns):\n",
    "    model_fit = fitting_returns_data(\n",
    "        'ff6_factors_19630701_20230131.csv',\n",
    "        io_day_1_lag,\n",
    "        model_feed_forward,\n",
    "        seed = seeds[i],\n",
    "        print_summary = False)[2]\n",
    "    alphas.append(model_fit.params[0])\n",
    "    if i%10 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k4Yfo9e5Iqze",
   "metadata": {
    "id": "k4Yfo9e5Iqze"
   },
   "outputs": [],
   "source": [
    "print(f'Highest return if we ignore robustness: {max(alphas)}')\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(alphas, bins=10, alpha=0.7, color='blue')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Histogram of Feed Forward Model Alphas')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OhpnfVxELsKF",
   "metadata": {
    "id": "OhpnfVxELsKF"
   },
   "source": [
    "The highest alpha we could get if we ignore robustness is 0.135. Simply taking the highest alpha from a bunch of random trials would cause us to falsely believe our model is better than it is. This will lead to the model's performance with real money on the market being far worse than our backtest expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cReqpTcKxjl0",
   "metadata": {
    "id": "cReqpTcKxjl0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
