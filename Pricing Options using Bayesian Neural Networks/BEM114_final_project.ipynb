{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e677baf",
   "metadata": {
    "id": "9e677baf"
   },
   "source": [
    "# Pricing Options using Bayesian Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18326965",
   "metadata": {
    "id": "18326965"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "\n",
    "# We'll suppress some warnings from tensorflow, but feel free to\n",
    "# comment out this line and see some of the efficiency gains we can get!\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Note: All io_fns and model_fns start with an 'io_' and 'model_' prefix\n",
    "# respectively\n",
    "\n",
    "\n",
    "\n",
    "def fitting_returns_data(data_path,\n",
    "                         io_fn,\n",
    "                         model_fn,\n",
    "                         seed = None,\n",
    "                         print_summary = True):\n",
    "    \"\"\"\n",
    "    Fits data to supplied model and provides returns and alpha estimates.\n",
    "\n",
    "        Parameters:\n",
    "            data_path (str): path to factor data with first column as date\n",
    "                6 factors, and the last column as risk free rate\n",
    "            io_fn (fn): a function to calculate input/output pairs for the\n",
    "                model fit\n",
    "            model_fn (fn): a function that takes in input data, output data,\n",
    "                and training and testing values, and returns the strategy\n",
    "                for each day, and the observed return for each day\n",
    "            seed (int): if given, the seed for the model fit will be set to\n",
    "                this value\n",
    "            print_summary (bool): if True, then the model won't print a summary\n",
    "                of the OLS fit for our strategy return.\n",
    "        Returns:\n",
    "            strat_df (pd.DataFrame): The weights we would give each factor\n",
    "                on each day\n",
    "            return_vector (list): The returns from our strategy on each day\n",
    "            model_OLS (RegressionResultsWrapper): A regression calculating the\n",
    "                alpha for our strategy\n",
    "    \"\"\"\n",
    "    # if given, set the random seed\n",
    "    if seed:\n",
    "        set_seed(seed)\n",
    "\n",
    "    # Load the data into a pandas DataFrame\n",
    "    data = pd.read_csv(data_path)\n",
    "    # Drop date and risk free rate\n",
    "    data = data.iloc[:, 1:7]\n",
    "\n",
    "    # Shift the data by one time step to create input/output pairs\n",
    "    X, y = io_fn(data)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Fit the model\n",
    "    strat_df, return_vector = model_fn(X, y, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Calculate alpha\n",
    "    y_ols = sm.add_constant(y)\n",
    "    model_OLS = sm.OLS(return_vector, y_ols).fit()\n",
    "    if print_summary:\n",
    "        print(model_OLS.summary())\n",
    "\n",
    "    return strat_df, return_vector, model_OLS\n",
    "\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    # Adding a fixed seed from this solution: https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras\n",
    "    # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    # 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.compat.v1.set_random_seed(seed_value)\n",
    "    return\n",
    "\n",
    "def io_day_1_lag(data):\n",
    "    # Create input output pairs, where the input is the previous day of data\n",
    "    # and the output is the current day of data.\n",
    "    X = data.shift(1).dropna().reset_index(drop=True)\n",
    "    y = data.dropna().iloc[1:,:].reset_index(drop=True)\n",
    "    return X, y\n",
    "\n",
    "def io_day_1_lag_second_order_input(data):\n",
    "    # Create input output pairs where input data includes second order interactions\n",
    "    X, y = io_day_1_lag(data)\n",
    "    cols = X.columns\n",
    "\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            col_name = cols[i] + cols[j]\n",
    "            col_values = X[cols[i]] * X[cols[j]]\n",
    "            X[col_name] = col_values\n",
    "    return X, y\n",
    "\n",
    "def model_feed_forward(X, y, X_train, y_train, X_val, y_val):\n",
    "    # Define the neural network model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(6, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Train the model, verbose = 0 means reports aren't printed\n",
    "    # at the end of each epoch\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=50,\n",
    "              validation_data=(X_val, y_val))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "\n",
    "    return predictions_to_returns(pred_df, y)\n",
    "\n",
    "def predictions_to_returns(pred_df, y):\n",
    "    # Given the predictions of each factor for each day, calculate our\n",
    "    # strategy for each day, and the returns for each day\n",
    "\n",
    "    # Apply our strategy to our predictions\n",
    "    strat_df = pred_df.apply(lambda row : max_predicted_factor_strat(row), axis = 1)\n",
    "\n",
    "    # Calculate our returns\n",
    "    return_vector = np.multiply(strat_df,np.asarray(y)).apply(sum, axis = 1)\n",
    "\n",
    "    return strat_df, return_vector\n",
    "\n",
    "\n",
    "def max_predicted_factor_strat(row):\n",
    "    # For each day, set our strategy to be the factor with\n",
    "    # the highest predicted return\n",
    "    max_pred_return = max(row)\n",
    "    row_list = [x == max_pred_return for x in row]\n",
    "    return pd.Series(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5357902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e470e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# SOURCE:\n",
    "# https://www.codearmo.com/python-tutorial/options-trading-black-scholes-model\n",
    "\n",
    "N = norm.cdf\n",
    "\n",
    "def BS_CALL(S, K, T, r, sigma):\n",
    "    d1 = (np.log(S/K) + (r + sigma**2/2)*T) / (sigma*np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    return S * N(d1) - K * np.exp(-r*T)* N(d2)\n",
    "\n",
    "def BS_PUT(S, K, T, r, sigma):\n",
    "    d1 = (np.log(S/K) + (r + sigma**2/2)*T) / (sigma*np.sqrt(T))\n",
    "    d2 = d1 - sigma* np.sqrt(T)\n",
    "    return K*np.exp(-r*T)*N(-d2) - S*N(-d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e8c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9b590803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/24/b7637qzn5pz9_l33fc30f4240000gn/T/ipykernel_28183/1956191067.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  close_df['prev_Close'] = close_df['Close'].shift(1)\n",
      "/var/folders/24/b7637qzn5pz9_l33fc30f4240000gn/T/ipykernel_28183/1956191067.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  close_df['prev_prev_Close'] = close_df['prev_Close'].shift(1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>prev_Close</th>\n",
       "      <th>prev_prev_Close</th>\n",
       "      <th>Ret</th>\n",
       "      <th>prev_Ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1985-03-01</td>\n",
       "      <td>180.660004</td>\n",
       "      <td>181.179993</td>\n",
       "      <td>179.630005</td>\n",
       "      <td>-0.002870</td>\n",
       "      <td>0.008629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1985-04-01</td>\n",
       "      <td>179.830002</td>\n",
       "      <td>180.660004</td>\n",
       "      <td>181.179993</td>\n",
       "      <td>-0.004594</td>\n",
       "      <td>-0.002870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1985-05-01</td>\n",
       "      <td>189.550003</td>\n",
       "      <td>179.830002</td>\n",
       "      <td>180.660004</td>\n",
       "      <td>0.054051</td>\n",
       "      <td>-0.004594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1985-06-01</td>\n",
       "      <td>191.850006</td>\n",
       "      <td>189.550003</td>\n",
       "      <td>179.830002</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>0.054051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1985-07-01</td>\n",
       "      <td>190.919998</td>\n",
       "      <td>191.850006</td>\n",
       "      <td>189.550003</td>\n",
       "      <td>-0.004848</td>\n",
       "      <td>0.012134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>4845.649902</td>\n",
       "      <td>4769.830078</td>\n",
       "      <td>4567.799805</td>\n",
       "      <td>0.015896</td>\n",
       "      <td>0.044229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>5096.270020</td>\n",
       "      <td>4845.649902</td>\n",
       "      <td>4769.830078</td>\n",
       "      <td>0.051721</td>\n",
       "      <td>0.015896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>5096.270020</td>\n",
       "      <td>4845.649902</td>\n",
       "      <td>0.031019</td>\n",
       "      <td>0.051721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>5035.689941</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>5096.270020</td>\n",
       "      <td>-0.041615</td>\n",
       "      <td>0.031019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>5266.950195</td>\n",
       "      <td>5035.689941</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>0.045924</td>\n",
       "      <td>-0.041615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Close   prev_Close  prev_prev_Close       Ret  prev_Ret\n",
       "2    1985-03-01   180.660004   181.179993       179.630005 -0.002870  0.008629\n",
       "3    1985-04-01   179.830002   180.660004       181.179993 -0.004594 -0.002870\n",
       "4    1985-05-01   189.550003   179.830002       180.660004  0.054051 -0.004594\n",
       "5    1985-06-01   191.850006   189.550003       179.830002  0.012134  0.054051\n",
       "6    1985-07-01   190.919998   191.850006       189.550003 -0.004848  0.012134\n",
       "..          ...          ...          ...              ...       ...       ...\n",
       "468  2024-01-01  4845.649902  4769.830078      4567.799805  0.015896  0.044229\n",
       "469  2024-02-01  5096.270020  4845.649902      4769.830078  0.051721  0.015896\n",
       "470  2024-03-01  5254.350098  5096.270020      4845.649902  0.031019  0.051721\n",
       "471  2024-04-01  5035.689941  5254.350098      5096.270020 -0.041615  0.031019\n",
       "472  2024-05-01  5266.950195  5035.689941      5254.350098  0.045924 -0.041615\n",
       "\n",
       "[471 rows x 6 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('^SPX.csv')\n",
    "\n",
    "close_df = df[['Date', 'Close']]\n",
    "close_df['prev_Close'] = close_df['Close'].shift(1)\n",
    "close_df['prev_prev_Close'] = close_df['prev_Close'].shift(1)\n",
    "\n",
    "close_df = close_df.dropna(subset=['prev_prev_Close'])\n",
    "close_df = close_df.astype({'Close': 'double', 'prev_Close': 'double'})\n",
    "\n",
    "close_df['Ret'] = close_df['Close'] / close_df['prev_Close'] - 1\n",
    "close_df['prev_Ret'] = close_df['prev_Close'] / close_df['prev_prev_Close'] - 1\n",
    "\n",
    "# Drop last row cause it is an incomplete month\n",
    "close_df.drop(close_df.tail(1).index, inplace=True)\n",
    "\n",
    "close_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a24ab759",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.from_numpy(close_df['prev_Ret'].values)\n",
    "x = torch.unsqueeze(x_data, dim=1)\n",
    "\n",
    "y_data = torch.from_numpy(close_df['Ret'].values)\n",
    "y = torch.unsqueeze(y_data, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bb36ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchbnn as bnn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4b8d419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=1, out_features=1000).double(),\n",
    "    nn.ReLU(),\n",
    "    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=1000, out_features=1).double(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2d2715a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "kl_weight = 0.01\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bfbfe22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- MSE : 0.06, KL : 0.71\n"
     ]
    }
   ],
   "source": [
    "for step in range(2000):\n",
    "    pre = model(x)\n",
    "    mse = mse_loss(pre, y)\n",
    "    kl = kl_loss(model)\n",
    "    cost = mse + kl_weight*kl\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print('- MSE : %2.2f, KL : %2.2f' % (mse.item(), kl.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "11bdfe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_result = np.array([model(x).data.numpy() for k in range(10000)])\n",
    "models_result = models_result[:,:,0]    \n",
    "models_result = models_result.T\n",
    "mean_values = np.array([models_result[i].mean() for i in range(len(models_result))])\n",
    "std_values = np.array([models_result[i].std() for i in range(len(models_result))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "737e056f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x_data.numpy(),mean_values,color='navy',lw=3,label='Predicted Mean Model')\n",
    "plt.fill_between(x_data.numpy().T[0],mean_values-3.0*std_values,mean_values+3.0*std_values,alpha=0.2,color='navy',label='99.7% confidence interval')\n",
    "plt.plot(x_data.numpy(),y_data.numpy(),'.',color='darkorange',markersize=4,label='Test set')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cd391d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b6967790>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_data.numpy(), y_data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1d716428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(x_data, bins=50, edgecolor='black')  # Adjust the number of bins as needed\n",
    "plt.title('Histogram of Monthly Returns')\n",
    "plt.xlabel('Monthly Return')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fb439a19",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchbnn' has no attribute 'plot_posterior'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[0;32m----> 2\u001b[0m bnn\u001b[38;5;241m.\u001b[39mplot_posterior(model, ax\u001b[38;5;241m=\u001b[39max, show_theta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchbnn' has no attribute 'plot_posterior'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bnn.plot_posterior(model, ax=ax, show_theta=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a924ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e972b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9fb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d733959a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90facca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1553e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edc9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36f9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede15bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137da1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be65bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "765ed3e4",
   "metadata": {
    "id": "765ed3e4"
   },
   "source": [
    "## Question 0 - Run the Neural Net and Interpet the alpha\n",
    "The following code fits a feed-forward neural net on all data, and prints a summary.\n",
    "Run the code and provide an interpretation of the alpha.\n",
    "\n",
    "```\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_1_lag,\n",
    "    model_feed_forward);\n",
    "```\n",
    "\n",
    "**Note**: The test-train split we're employing in these model fits is faulty. We use data from the future to predict our model, so all results here have some level of overfitting. However, I separately fit the models by only using past data, and we were able to get similar results, so the results still seem robust to the potential overfitting.\n",
    "\n",
    "### Question 0 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f49a61a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f49a61a",
    "outputId": "2f1da065-7f42-4039-e7be-173330580c6c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "375/375 [==============================] - 4s 6ms/step - loss: 0.4313 - val_loss: 0.4482\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.4189 - val_loss: 0.4461\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 3s 9ms/step - loss: 0.4163 - val_loss: 0.4463\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4147 - val_loss: 0.4461\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.4133 - val_loss: 0.4451\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4119 - val_loss: 0.4450\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.4110 - val_loss: 0.4453\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4105 - val_loss: 0.4450\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4098 - val_loss: 0.4458\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4092 - val_loss: 0.4461\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4085 - val_loss: 0.4469\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4079 - val_loss: 0.4468\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.4471\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4070 - val_loss: 0.4461\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4067 - val_loss: 0.4463\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4060 - val_loss: 0.4456\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4054 - val_loss: 0.4468\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4050 - val_loss: 0.4494\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4049 - val_loss: 0.4459\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4042 - val_loss: 0.4466\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4041 - val_loss: 0.4468\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4033 - val_loss: 0.4459\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4035 - val_loss: 0.4476\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4029 - val_loss: 0.4480\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4024 - val_loss: 0.4501\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4027 - val_loss: 0.4473\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4019 - val_loss: 0.4478\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4018 - val_loss: 0.4484\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4014 - val_loss: 0.4497\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4010 - val_loss: 0.4494\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4010 - val_loss: 0.4493\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4002 - val_loss: 0.4489\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4004 - val_loss: 0.4507\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3996 - val_loss: 0.4507\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3995 - val_loss: 0.4493\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3993 - val_loss: 0.4489\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3990 - val_loss: 0.4481\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3984 - val_loss: 0.4498\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3990 - val_loss: 0.4498\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3985 - val_loss: 0.4502\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3980 - val_loss: 0.4514\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3977 - val_loss: 0.4513\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3971 - val_loss: 0.4494\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3972 - val_loss: 0.4515\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3969 - val_loss: 0.4506\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3962 - val_loss: 0.4520\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3961 - val_loss: 0.4517\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3960 - val_loss: 0.4505\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3961 - val_loss: 0.4529\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3957 - val_loss: 0.4543\n",
      "469/469 [==============================] - 1s 1ms/step\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.147\n",
      "Model:                            OLS   Adj. R-squared:                  0.147\n",
      "Method:                 Least Squares   F-statistic:                     431.8\n",
      "Date:                Sun, 19 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        03:32:53   Log-Likelihood:                -15710.\n",
      "No. Observations:               14998   AIC:                         3.143e+04\n",
      "Df Residuals:                   14991   BIC:                         3.149e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1232      0.006     21.787      0.000       0.112       0.134\n",
      "Mkt-RF         0.2080      0.006     34.344      0.000       0.196       0.220\n",
      "SMB            0.0161      0.011      1.461      0.144      -0.006       0.038\n",
      "HML            0.1985      0.013     15.628      0.000       0.174       0.223\n",
      "RMW            0.1350      0.015      8.884      0.000       0.105       0.165\n",
      "CMA            0.0336      0.020      1.724      0.085      -0.005       0.072\n",
      "MOM            0.3153      0.008     39.919      0.000       0.300       0.331\n",
      "==============================================================================\n",
      "Omnibus:                     4889.878   Durbin-Watson:                   1.938\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           291002.805\n",
      "Skew:                           0.748   Prob(JB):                         0.00\n",
      "Kurtosis:                      24.527   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "COMMON_SEED = 519\n",
    "\n",
    "# Calculate returns and alphas using feed forward neural net model\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_1_lag,\n",
    "    model_feed_forward,\n",
    "    seed = COMMON_SEED);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d6dd8",
   "metadata": {
    "id": "d26d6dd8"
   },
   "source": [
    "The alpha value of 0.1232 is positive and significant at the 1% level, indicating that using Neural Networks to identify relationships between the six factors is a profitable hedge fund strategy.\n",
    "\n",
    "## Question 1 - Comparing Neural Net to OLS\n",
    "\n",
    "### 1ai [20 points] Set a linear activation function\n",
    "For the feed-forward neural net that is fit above, the `model_feed_forward` function sets up activation across hidden layers with the following code:\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(6,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(6, activation='linear'))\n",
    "```\n",
    "\n",
    "Write a new function called `model_feed_forward_linear`, which changes the `'relu'` parameter to `'linear'`.\n",
    "\n",
    "\n",
    "Calculate reults for the neural net with linear activation. How does this compare to the ReLU activation from Question 0?\n",
    "\n",
    "### Solution 1ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1106235",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1106235",
    "outputId": "47166348-22b3-46b5-8089-9bad93dd7257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 2ms/step - loss: 0.4409 - val_loss: 0.4450\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4226 - val_loss: 0.4450\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4217 - val_loss: 0.4446\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4206 - val_loss: 0.4527\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4213 - val_loss: 0.4448\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4201 - val_loss: 0.4466\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4205 - val_loss: 0.4462\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4211 - val_loss: 0.4473\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4197 - val_loss: 0.4479\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4198 - val_loss: 0.4540\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4209 - val_loss: 0.4475\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4196 - val_loss: 0.4488\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4201 - val_loss: 0.4453\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4195 - val_loss: 0.4537\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4204 - val_loss: 0.4442\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4196 - val_loss: 0.4441\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4195 - val_loss: 0.4467\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4199 - val_loss: 0.4453\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4191 - val_loss: 0.4422\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4189 - val_loss: 0.4459\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4194 - val_loss: 0.4468\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4189 - val_loss: 0.4430\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4188 - val_loss: 0.4451\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4190 - val_loss: 0.4441\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4189 - val_loss: 0.4444\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4189 - val_loss: 0.4430\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4187 - val_loss: 0.4457\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4185 - val_loss: 0.4434\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4187 - val_loss: 0.4451\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4184 - val_loss: 0.4433\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4184 - val_loss: 0.4434\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4183 - val_loss: 0.4438\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4183 - val_loss: 0.4442\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4180 - val_loss: 0.4450\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4182 - val_loss: 0.4440\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4180 - val_loss: 0.4446\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4178 - val_loss: 0.4427\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4178 - val_loss: 0.4441\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4181 - val_loss: 0.4438\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4180 - val_loss: 0.4440\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4180 - val_loss: 0.4440\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4179 - val_loss: 0.4443\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4177 - val_loss: 0.4433\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4178 - val_loss: 0.4435\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4178 - val_loss: 0.4429\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4178 - val_loss: 0.4445\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4177 - val_loss: 0.4437\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4176 - val_loss: 0.4432\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4177 - val_loss: 0.4438\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4178 - val_loss: 0.4435\n",
      "469/469 [==============================] - 1s 1ms/step\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.141\n",
      "Model:                            OLS   Adj. R-squared:                  0.141\n",
      "Method:                 Least Squares   F-statistic:                     410.2\n",
      "Date:                Sun, 19 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        03:33:42   Log-Likelihood:                -15553.\n",
      "No. Observations:               14998   AIC:                         3.112e+04\n",
      "Df Residuals:                   14991   BIC:                         3.117e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0950      0.006     16.980      0.000       0.084       0.106\n",
      "Mkt-RF         0.1620      0.006     27.029      0.000       0.150       0.174\n",
      "SMB            0.0867      0.011      7.945      0.000       0.065       0.108\n",
      "HML            0.1878      0.013     14.942      0.000       0.163       0.212\n",
      "RMW            0.1302      0.015      8.658      0.000       0.101       0.160\n",
      "CMA            0.1382      0.019      7.156      0.000       0.100       0.176\n",
      "MOM            0.3216      0.008     41.150      0.000       0.306       0.337\n",
      "==============================================================================\n",
      "Omnibus:                     3648.074   Durbin-Watson:                   2.018\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           188624.493\n",
      "Skew:                          -0.292   Prob(JB):                         0.00\n",
      "Kurtosis:                      20.364   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           0      1      2      3      4      5\n",
       " 0       True  False  False  False  False  False\n",
       " 1      False  False  False  False  False   True\n",
       " 2      False  False  False  False  False   True\n",
       " 3       True  False  False  False  False  False\n",
       " 4      False  False  False  False   True  False\n",
       " ...      ...    ...    ...    ...    ...    ...\n",
       " 14993  False  False  False  False  False   True\n",
       " 14994  False  False   True  False  False  False\n",
       " 14995   True  False  False  False  False  False\n",
       " 14996   True  False  False  False  False  False\n",
       " 14997  False  False  False  False  False   True\n",
       " \n",
       " [14998 rows x 6 columns],\n",
       " 0        0.79\n",
       " 1        0.41\n",
       " 2        0.07\n",
       " 3       -0.63\n",
       " 4       -0.01\n",
       "          ... \n",
       " 14993    0.14\n",
       " 14994    0.01\n",
       " 14995    0.36\n",
       " 14996   -1.38\n",
       " 14997   -0.70\n",
       " Length: 14998, dtype: float64,\n",
       " <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7b0106446ef0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define linear feed-forward neural net based on model_feed_forward\n",
    "def model_feed_forward_linear(X, y, X_train, y_train, X_val, y_val):\n",
    "    # Define the neural network model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='linear', input_shape=(X.shape[1],)))\n",
    "    model.add(Dense(16, activation='linear'))\n",
    "    model.add(Dense(6, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Train the model, verbose = 0 means reports aren't printed\n",
    "    # at the end of each epoch\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=50,\n",
    "              validation_data=(X_val, y_val))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "\n",
    "    return predictions_to_returns(pred_df, y)\n",
    "\n",
    "# Calculate returns and alphas from linear neural net\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_1_lag,\n",
    "    model_feed_forward_linear,\n",
    "    seed = COMMON_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XzogcR_otKvx",
   "metadata": {
    "id": "XzogcR_otKvx"
   },
   "source": [
    "The ReLU alpha of 0.1232 is higher than the linear alpha of 0.0950, while both are statistically significant at the 1% level. This indicates that the ReLU activation function better supports neural networks to identify profitable relationships between factors, though we may need more in-depth experiments to conclude this convincingly.\n",
    "\n",
    "### 1aii [10 points] Compare with linear model\n",
    "\n",
    "The following code defines a function called `model_linear_fit`, which fits a linear model on y_train and X_train and outputs the predicted return given `X`, called `pred_df`. The function then calculates the  returns of the linear model by running `predictions_to_returns(pred_df, y)`.\n",
    "\n",
    "Since the neural net is only fitting on linear relationships, we should see similar results across the linear neural net and the linear OLS model here.\n",
    "\n",
    "Calculate reults for the linear model below. How does the alpha compare to the linear neural net?  \n",
    "\n",
    "### Solution 1aii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d97083",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09d97083",
    "outputId": "3f56ad57-f39e-45db-a221-b71b933066a0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.127\n",
      "Model:                            OLS   Adj. R-squared:                  0.127\n",
      "Method:                 Least Squares   F-statistic:                     363.2\n",
      "Date:                Sun, 19 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        03:33:44   Log-Likelihood:                -15513.\n",
      "No. Observations:               14998   AIC:                         3.104e+04\n",
      "Df Residuals:                   14991   BIC:                         3.109e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0904      0.006     16.202      0.000       0.079       0.101\n",
      "Mkt-RF         0.1371      0.006     22.937      0.000       0.125       0.149\n",
      "SMB            0.1647      0.011     15.134      0.000       0.143       0.186\n",
      "HML            0.2111      0.013     16.842      0.000       0.187       0.236\n",
      "RMW            0.1475      0.015      9.833      0.000       0.118       0.177\n",
      "CMA            0.1534      0.019      7.967      0.000       0.116       0.191\n",
      "MOM            0.2758      0.008     35.382      0.000       0.261       0.291\n",
      "==============================================================================\n",
      "Omnibus:                     3960.117   Durbin-Watson:                   2.005\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           262916.099\n",
      "Skew:                          -0.336   Prob(JB):                         0.00\n",
      "Kurtosis:                      23.501   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           0      1      2      3      4      5\n",
       " 0       True  False  False  False  False  False\n",
       " 1      False  False  False  False  False   True\n",
       " 2      False  False  False  False  False   True\n",
       " 3      False   True  False  False  False  False\n",
       " 4      False  False  False  False   True  False\n",
       " ...      ...    ...    ...    ...    ...    ...\n",
       " 14993  False  False  False  False  False   True\n",
       " 14994  False  False   True  False  False  False\n",
       " 14995   True  False  False  False  False  False\n",
       " 14996   True  False  False  False  False  False\n",
       " 14997  False  False  False  False  False   True\n",
       " \n",
       " [14998 rows x 6 columns],\n",
       " 0        0.79\n",
       " 1        0.41\n",
       " 2        0.07\n",
       " 3        0.07\n",
       " 4       -0.01\n",
       "          ... \n",
       " 14993    0.14\n",
       " 14994    0.01\n",
       " 14995    0.36\n",
       " 14996   -1.38\n",
       " 14997   -0.70\n",
       " Length: 14998, dtype: float64,\n",
       " <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7b0109f955a0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define linear OLS model\n",
    "## Linear Model Fit\n",
    "def model_linear_fit(X, y, X_train, y_train, X_val, y_val):\n",
    "    model_OLS = sm.OLS(y_train, X_train).fit()\n",
    "    # Make predictions\n",
    "    predictions = model_OLS.predict(X)\n",
    "\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "\n",
    "    return predictions_to_returns(pred_df, y)\n",
    "\n",
    "# Calculate returns and alphas using linear OLS model\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_1_lag,\n",
    "    model_linear_fit,\n",
    "    seed = COMMON_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c7ff9",
   "metadata": {
    "id": "746c7ff9"
   },
   "source": [
    "The linear model alpha of 0.0904 is very similar the neural network model alpha of 0.0950, which is as expected. Both are statistically significant at the 1% level.\n",
    "\n",
    "### 1b [20 points] Include Interaction Terms\n",
    "Linear models don't account for any interaction effects. In order to account for an interaction we can add input variables that give the product of factor returns on each day. This would be analogous to adding a interaction term to a linear model.\n",
    "\n",
    "Using the io_ function `io_day_1_lag_second_order_input`, and your `model_linear_fit` function, calculate returns while including second order inputs.\n",
    "\n",
    "Write a new io_ function `io_day_lag_third_order_input`, to also include third order fits in your input data.\n",
    "\n",
    "How do these models compare to the ReLU alpha?\n",
    "\n",
    "\n",
    "### Solution 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2921ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e2921ed",
    "outputId": "1b3704be-189e-452b-ced3-82585affe7c0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.164\n",
      "Model:                            OLS   Adj. R-squared:                  0.163\n",
      "Method:                 Least Squares   F-statistic:                     489.2\n",
      "Date:                Sun, 19 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        03:33:49   Log-Likelihood:                -16116.\n",
      "No. Observations:               14998   AIC:                         3.225e+04\n",
      "Df Residuals:                   14991   BIC:                         3.230e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0965      0.006     16.615      0.000       0.085       0.108\n",
      "Mkt-RF         0.2727      0.006     43.824      0.000       0.261       0.285\n",
      "SMB            0.1154      0.011     10.188      0.000       0.093       0.138\n",
      "HML            0.2557      0.013     19.601      0.000       0.230       0.281\n",
      "RMW            0.1325      0.016      8.485      0.000       0.102       0.163\n",
      "CMA            0.1257      0.020      6.270      0.000       0.086       0.165\n",
      "MOM            0.2588      0.008     31.892      0.000       0.243       0.275\n",
      "==============================================================================\n",
      "Omnibus:                     3804.967   Durbin-Watson:                   2.039\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           304893.099\n",
      "Skew:                          -0.010   Prob(JB):                         0.00\n",
      "Kurtosis:                      25.088   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           0      1      2      3      4      5\n",
       " 0       True  False  False  False  False  False\n",
       " 1      False  False  False  False  False   True\n",
       " 2      False  False  False  False  False   True\n",
       " 3       True  False  False  False  False  False\n",
       " 4      False  False  False  False   True  False\n",
       " ...      ...    ...    ...    ...    ...    ...\n",
       " 14993  False  False  False  False  False   True\n",
       " 14994  False  False   True  False  False  False\n",
       " 14995   True  False  False  False  False  False\n",
       " 14996   True  False  False  False  False  False\n",
       " 14997  False  False  False  False  False   True\n",
       " \n",
       " [14998 rows x 6 columns],\n",
       " 0        0.79\n",
       " 1        0.41\n",
       " 2        0.07\n",
       " 3       -0.63\n",
       " 4       -0.01\n",
       "          ... \n",
       " 14993    0.14\n",
       " 14994    0.01\n",
       " 14995    0.36\n",
       " 14996   -1.38\n",
       " 14997   -0.70\n",
       " Length: 14998, dtype: float64,\n",
       " <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7b0109f95ba0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate returns and alphas using linear OLS model\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_1_lag_second_order_input,\n",
    "    model_linear_fit,\n",
    "    seed = COMMON_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd49b62a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd49b62a",
    "outputId": "d9995bcb-0f62-4d31-acf7-c50a85f21fc5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.120\n",
      "Model:                            OLS   Adj. R-squared:                  0.120\n",
      "Method:                 Least Squares   F-statistic:                     340.7\n",
      "Date:                Sun, 19 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        03:33:53   Log-Likelihood:                -16045.\n",
      "No. Observations:               14998   AIC:                         3.210e+04\n",
      "Df Residuals:                   14991   BIC:                         3.216e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1240      0.006     21.447      0.000       0.113       0.135\n",
      "Mkt-RF         0.1922      0.006     31.031      0.000       0.180       0.204\n",
      "SMB            0.0399      0.011      3.536      0.000       0.018       0.062\n",
      "HML            0.1763      0.013     13.579      0.000       0.151       0.202\n",
      "RMW            0.1619      0.016     10.419      0.000       0.131       0.192\n",
      "CMA            0.0831      0.020      4.165      0.000       0.044       0.122\n",
      "MOM            0.2760      0.008     34.172      0.000       0.260       0.292\n",
      "==============================================================================\n",
      "Omnibus:                     4135.930   Durbin-Watson:                   1.884\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           268573.934\n",
      "Skew:                           0.431   Prob(JB):                         0.00\n",
      "Kurtosis:                      23.713   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           0      1      2      3      4      5\n",
       " 0      False  False  False  False   True  False\n",
       " 1      False  False  False  False  False   True\n",
       " 2      False  False  False  False  False   True\n",
       " 3       True  False  False  False  False  False\n",
       " 4      False  False  False  False   True  False\n",
       " ...      ...    ...    ...    ...    ...    ...\n",
       " 14993  False  False  False  False  False   True\n",
       " 14994  False  False   True  False  False  False\n",
       " 14995  False   True  False  False  False  False\n",
       " 14996  False   True  False  False  False  False\n",
       " 14997  False  False  False  False  False   True\n",
       " \n",
       " [14998 rows x 6 columns],\n",
       " 0       -0.21\n",
       " 1        0.41\n",
       " 2        0.07\n",
       " 3       -0.63\n",
       " 4       -0.01\n",
       "          ... \n",
       " 14993    0.14\n",
       " 14994    0.01\n",
       " 14995    0.32\n",
       " 14996    0.05\n",
       " 14997   -0.70\n",
       " Length: 14998, dtype: float64,\n",
       " <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7b0109f95390>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create input output pairs where input data includes second order interactions\n",
    "\n",
    "# This is actually third order as described in the question. The outputs are not\n",
    "# similar to the sanity checks that professor Sinclair wrote for us.\n",
    "# def io_day_lag_third_order_input(data):\n",
    "#     X, y = io_day_1_lag(data)\n",
    "#     cols = X.columns\n",
    "\n",
    "#     X, y = io_day_1_lag_second_order_input(data)\n",
    "\n",
    "#     for i in range(len(cols)):\n",
    "#         for j in range(i+1, len(cols)):\n",
    "#           for k in range(j+1, len(cols)):\n",
    "#             col_name = cols[i] + cols[j] + cols[k]\n",
    "#             col_values = X[cols[i]] * X[cols[j]] * X[cols[k]]\n",
    "#             X[col_name] = col_values\n",
    "\n",
    "#     return X, y\n",
    "\n",
    "# This is fourth order. The outputs agree very closely with professor Sinclair's\n",
    "# sanity checks.\n",
    "def io_day_lag_third_order_input(data):\n",
    "    # Create input output pairs where input data includes second order interactions\n",
    "    X, y = io_day_1_lag_second_order_input(data)\n",
    "    cols = X.columns\n",
    "\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            col_name = cols[i] + cols[j]\n",
    "            col_values = X[cols[i]] * X[cols[j]]\n",
    "            X[col_name] = col_values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Calculate returns and alphas using third order interactions\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_lag_third_order_input,\n",
    "    model_linear_fit,\n",
    "    seed = COMMON_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c46887",
   "metadata": {
    "id": "b1c46887"
   },
   "source": [
    "The second order linear model has an alpha of 0.0965, which is lower than the ReLU alpha of 0.1232, but is significant at the 1% level. On the other hand, the third order linear model that we implemented as fourth order to be similar to the desired output has alpha of 0.1240, which is very similar to the ReLU alpha. It is also statistically significant at the 1% level.\n",
    "\n",
    "## Question 2 - Adding Time Lag Parameters\n",
    "### 2 [25 points] Lag Time Parameters\n",
    "Our current neural net only uses the past 1 day of data. For time series data, including more lag days can be useful. For example, if a factor return is high for 2 consecutive days, that may be more informative than just knowing that the return was only high for the previous day.\n",
    "\n",
    "In this question, we will simply add a new column to our input data for each lagged data. This is analogous to an AutoRegressive Model, which is a popular financial engineering tool, [see this textbook](https://link.springer.com/book/10.1007/978-1-4939-2614-5). In the Neural Net literature, a Recurrent Neural Net is a common tool for more directly accounting for time lagged data directly in the neural net architecture, but the lagged model gets us a good amount of the way there!\n",
    "\n",
    "Starting from the `io_day_1_lag` function write an `io_day_5_lag` function, which adds to the input dataframe 5 days of lagged data per factor. This will mean your input data will now have 6*5 = 30 columns instead of 6 columns.\n",
    "\n",
    "When complete, run the following and compare results to the original ReLU model from Question 0. How do the results compare? Should we continue to to pursue incorporating lag effects in our analysis of this data?\n",
    "\n",
    "```\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_5_lag,\n",
    "    model_feed_forward);\n",
    " ```\n",
    "\n",
    " ### Question 2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ab4fd43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ab4fd43",
    "outputId": "178092e2-c063-4d0c-ef79-ad37fe64aaa8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4547 - val_loss: 0.4522\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4241 - val_loss: 0.4499\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4196 - val_loss: 0.4489\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4159 - val_loss: 0.4481\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4123 - val_loss: 0.4488\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4482\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4051 - val_loss: 0.4479\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4017 - val_loss: 0.4482\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3992 - val_loss: 0.4505\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3956 - val_loss: 0.4498\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3931 - val_loss: 0.4540\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3903 - val_loss: 0.4548\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3883 - val_loss: 0.4583\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3858 - val_loss: 0.4586\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3833 - val_loss: 0.4583\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3816 - val_loss: 0.4601\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3795 - val_loss: 0.4622\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3775 - val_loss: 0.4631\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3750 - val_loss: 0.4630\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3727 - val_loss: 0.4625\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3709 - val_loss: 0.4664\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3696 - val_loss: 0.4667\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3672 - val_loss: 0.4704\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3664 - val_loss: 0.4656\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3646 - val_loss: 0.4686\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3629 - val_loss: 0.4725\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3614 - val_loss: 0.4721\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3605 - val_loss: 0.4719\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3591 - val_loss: 0.4699\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3580 - val_loss: 0.4756\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3567 - val_loss: 0.4749\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3555 - val_loss: 0.4768\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3544 - val_loss: 0.4743\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3534 - val_loss: 0.4767\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3519 - val_loss: 0.4768\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3510 - val_loss: 0.4765\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3501 - val_loss: 0.4786\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3495 - val_loss: 0.4818\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3475 - val_loss: 0.4800\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3463 - val_loss: 0.4844\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3463 - val_loss: 0.4818\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3453 - val_loss: 0.4808\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3439 - val_loss: 0.4903\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3438 - val_loss: 0.4897\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3431 - val_loss: 0.4884\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3428 - val_loss: 0.4884\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3419 - val_loss: 0.4907\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3410 - val_loss: 0.4896\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3401 - val_loss: 0.4939\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3396 - val_loss: 0.4891\n",
      "469/469 [==============================] - 1s 1ms/step\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.292\n",
      "Model:                            OLS   Adj. R-squared:                  0.292\n",
      "Method:                 Least Squares   F-statistic:                     1031.\n",
      "Date:                Sun, 19 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        03:34:42   Log-Likelihood:                -16382.\n",
      "No. Observations:               14994   AIC:                         3.278e+04\n",
      "Df Residuals:                   14987   BIC:                         3.283e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1574      0.006     26.605      0.000       0.146       0.169\n",
      "Mkt-RF         0.4712      0.006     74.362      0.000       0.459       0.484\n",
      "SMB           -0.0452      0.012     -3.916      0.000      -0.068      -0.023\n",
      "HML            0.1372      0.013     10.330      0.000       0.111       0.163\n",
      "RMW            0.0766      0.016      4.817      0.000       0.045       0.108\n",
      "CMA            0.0696      0.020      3.407      0.001       0.030       0.110\n",
      "MOM            0.1695      0.008     20.519      0.000       0.153       0.186\n",
      "==============================================================================\n",
      "Omnibus:                     7028.886   Durbin-Watson:                   1.714\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           145900.245\n",
      "Skew:                           1.764   Prob(JB):                         0.00\n",
      "Kurtosis:                      17.869   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           0      1      2      3      4      5\n",
       " 0       True  False  False  False  False  False\n",
       " 1      False  False  False  False  False   True\n",
       " 2       True  False  False  False  False  False\n",
       " 3       True  False  False  False  False  False\n",
       " 4       True  False  False  False  False  False\n",
       " ...      ...    ...    ...    ...    ...    ...\n",
       " 14989  False  False   True  False  False  False\n",
       " 14990  False  False  False  False  False   True\n",
       " 14991   True  False  False  False  False  False\n",
       " 14992   True  False  False  False  False  False\n",
       " 14993  False  False  False  False  False   True\n",
       " \n",
       " [14994 rows x 6 columns],\n",
       " 0        0.45\n",
       " 1        0.16\n",
       " 2       -0.16\n",
       " 3       -0.12\n",
       " 4       -0.62\n",
       "          ... \n",
       " 14989    0.65\n",
       " 14990   -1.23\n",
       " 14991    0.36\n",
       " 14992   -1.38\n",
       " 14993   -0.70\n",
       " Length: 14994, dtype: float64,\n",
       " <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7b01060fe170>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def io_day_5_lag(data):\n",
    "    X = data.shift(1).add_suffix('_lag1')\n",
    "    for i in range(2, 6):\n",
    "        shifted_data = data.shift(i).add_suffix('_lag{}'.format(i))\n",
    "        X = pd.concat([X, shifted_data], axis=1)\n",
    "    X = X.dropna().reset_index(drop=True)\n",
    "    y = data.iloc[5:,:].reset_index(drop=True)\n",
    "    return X, y\n",
    "\n",
    "# Calculate returns and alphas using the feed forward neural net with\n",
    "# five day lagged input variables.\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_5_lag,\n",
    "    model_feed_forward,\n",
    "    seed = COMMON_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84cfba",
   "metadata": {
    "id": "3c84cfba"
   },
   "source": [
    "The 5 day lag feed-forward neural network model has an alpha of 0.1574, which is larger than the 1 day lag ReLU model alpha of 0.1232 and statistically significant at the 1% level. This indicates that we should continue to incorporate lag effects into our models, and supports the intuitive notion that giving the model factor information from days earlier than the previous day will improve the performance.\n",
    "\n",
    "## Question 3 - Investigating Potentials for P-Hacking\n",
    "### 3 [25 points] Randomness in Alphas\n",
    "\n",
    "Neural  nets are fit via a Stochastic Gradient Descent. This implies that there is inherent randomness in any fit of the model. One good way to account for this noise in your model is to refit the model multiple times and observe the distribution. It's more accurate to report the median or mean of these estimates, although it can be hard to tell if a paper/report has cherry-picked the best result in this way.\n",
    "\n",
    "Using the `io_day_1_lag`, and `model_feed_forward` settings, rerun the model 100 times and get a distribution for the alpha given. (You can use the `seed` parameter in the `fitting_returns_data` function if you want to be able to reproduce a given high return.)\n",
    "\n",
    "Set `print_summary = False` in `fitting_returns_data` in order to avoid large amounts of output.\n",
    "\n",
    "What's the highest return you could get if you were to ignore the importance of the robustness of a model result? What would be a downside of reporting a result like this?\n",
    "\n",
    "Note: running the model 100 times may take awhile (over an hour on Google Colab). Debug your code before attempting the 100 cycles.\n",
    "\n",
    "### Question 3 Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7000f3c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7000f3c5",
    "outputId": "4fbe5e39-6129-4396-ebe2-3eca0d923791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4060 - val_loss: 0.4426\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4059 - val_loss: 0.4442\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4055 - val_loss: 0.4429\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4054 - val_loss: 0.4430\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4050 - val_loss: 0.4445\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4428\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4441\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4047 - val_loss: 0.4440\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4037 - val_loss: 0.4435\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4041 - val_loss: 0.4429\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4030 - val_loss: 0.4439\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4031 - val_loss: 0.4449\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4032 - val_loss: 0.4440\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4027 - val_loss: 0.4440\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4024 - val_loss: 0.4457\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4022 - val_loss: 0.4454\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4019 - val_loss: 0.4475\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4016 - val_loss: 0.4450\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4010 - val_loss: 0.4446\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4008 - val_loss: 0.4462\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4010 - val_loss: 0.4451\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4006 - val_loss: 0.4449\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "50\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4269 - val_loss: 0.4482\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4196 - val_loss: 0.4461\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4170 - val_loss: 0.4456\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4150 - val_loss: 0.4459\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4135 - val_loss: 0.4454\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4132 - val_loss: 0.4448\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4122 - val_loss: 0.4450\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4115 - val_loss: 0.4443\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4445\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4104 - val_loss: 0.4447\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4097 - val_loss: 0.4447\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4094 - val_loss: 0.4449\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4089 - val_loss: 0.4443\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4450\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4078 - val_loss: 0.4449\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4078 - val_loss: 0.4448\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4073 - val_loss: 0.4449\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4067 - val_loss: 0.4459\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4067 - val_loss: 0.4458\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4461\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4458\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4053 - val_loss: 0.4452\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4051 - val_loss: 0.4452\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4047 - val_loss: 0.4446\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4044 - val_loss: 0.4464\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4459\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4466\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4034 - val_loss: 0.4461\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4032 - val_loss: 0.4459\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4030 - val_loss: 0.4460\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4030 - val_loss: 0.4459\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4022 - val_loss: 0.4449\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4023 - val_loss: 0.4468\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4021 - val_loss: 0.4458\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4016 - val_loss: 0.4481\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4014 - val_loss: 0.4471\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4013 - val_loss: 0.4458\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4012 - val_loss: 0.4471\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4005 - val_loss: 0.4481\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4004 - val_loss: 0.4454\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4005 - val_loss: 0.4462\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4000 - val_loss: 0.4490\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4001 - val_loss: 0.4473\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3997 - val_loss: 0.4496\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3996 - val_loss: 0.4475\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3989 - val_loss: 0.4477\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3986 - val_loss: 0.4477\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3985 - val_loss: 0.4487\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3980 - val_loss: 0.4496\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3982 - val_loss: 0.4492\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4222 - val_loss: 0.4732\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.4705\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4120 - val_loss: 0.4685\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4104 - val_loss: 0.4676\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4093 - val_loss: 0.4684\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4082 - val_loss: 0.4689\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4079 - val_loss: 0.4666\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4073 - val_loss: 0.4671\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4653\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4057 - val_loss: 0.4661\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4050 - val_loss: 0.4654\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4045 - val_loss: 0.4668\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4038 - val_loss: 0.4674\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4035 - val_loss: 0.4662\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4676\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4024 - val_loss: 0.4647\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4021 - val_loss: 0.4659\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4017 - val_loss: 0.4655\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4011 - val_loss: 0.4669\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4006 - val_loss: 0.4699\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4005 - val_loss: 0.4656\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4004 - val_loss: 0.4648\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3997 - val_loss: 0.4670\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3996 - val_loss: 0.4669\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3995 - val_loss: 0.4673\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3990 - val_loss: 0.4676\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3987 - val_loss: 0.4689\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3986 - val_loss: 0.4667\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3983 - val_loss: 0.4692\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3981 - val_loss: 0.4662\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3976 - val_loss: 0.4685\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3977 - val_loss: 0.4676\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3975 - val_loss: 0.4672\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3971 - val_loss: 0.4676\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3968 - val_loss: 0.4665\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3964 - val_loss: 0.4697\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3963 - val_loss: 0.4718\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3963 - val_loss: 0.4664\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3961 - val_loss: 0.4693\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3956 - val_loss: 0.4674\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3954 - val_loss: 0.4687\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3952 - val_loss: 0.4676\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3949 - val_loss: 0.4707\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3951 - val_loss: 0.4669\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3946 - val_loss: 0.4706\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3946 - val_loss: 0.4689\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3942 - val_loss: 0.4698\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3942 - val_loss: 0.4696\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3940 - val_loss: 0.4704\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3938 - val_loss: 0.4705\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4319 - val_loss: 0.4364\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4230 - val_loss: 0.4340\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4204 - val_loss: 0.4322\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4184 - val_loss: 0.4316\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4171 - val_loss: 0.4306\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4159 - val_loss: 0.4302\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4149 - val_loss: 0.4297\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4141 - val_loss: 0.4304\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4133 - val_loss: 0.4306\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4127 - val_loss: 0.4303\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4120 - val_loss: 0.4302\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4118 - val_loss: 0.4295\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4112 - val_loss: 0.4302\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4301\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4104 - val_loss: 0.4309\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4306\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4095 - val_loss: 0.4298\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4094 - val_loss: 0.4307\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4091 - val_loss: 0.4304\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4084 - val_loss: 0.4309\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4082 - val_loss: 0.4328\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4312\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4304\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4072 - val_loss: 0.4313\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4308\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4065 - val_loss: 0.4316\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4062 - val_loss: 0.4320\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4059 - val_loss: 0.4342\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4057 - val_loss: 0.4311\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4055 - val_loss: 0.4346\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4050 - val_loss: 0.4315\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4048 - val_loss: 0.4347\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4049 - val_loss: 0.4333\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4044 - val_loss: 0.4359\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4039 - val_loss: 0.4338\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4038 - val_loss: 0.4342\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4033 - val_loss: 0.4361\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4031 - val_loss: 0.4357\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4034 - val_loss: 0.4337\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4337\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4024 - val_loss: 0.4330\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4023 - val_loss: 0.4346\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4022 - val_loss: 0.4366\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4021 - val_loss: 0.4343\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4016 - val_loss: 0.4360\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4011 - val_loss: 0.4377\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4010 - val_loss: 0.4346\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4010 - val_loss: 0.4389\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4006 - val_loss: 0.4362\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4005 - val_loss: 0.4389\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4315 - val_loss: 0.4270\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4245 - val_loss: 0.4238\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4221 - val_loss: 0.4241\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4205 - val_loss: 0.4236\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4192 - val_loss: 0.4241\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4184 - val_loss: 0.4228\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4181 - val_loss: 0.4228\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4168 - val_loss: 0.4225\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4163 - val_loss: 0.4233\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4156 - val_loss: 0.4217\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4149 - val_loss: 0.4224\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4149 - val_loss: 0.4217\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.4211\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4136 - val_loss: 0.4225\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4135 - val_loss: 0.4209\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4130 - val_loss: 0.4210\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4128 - val_loss: 0.4211\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4122 - val_loss: 0.4215\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4120 - val_loss: 0.4218\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4116 - val_loss: 0.4212\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4111 - val_loss: 0.4210\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4222\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4102 - val_loss: 0.4209\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4100 - val_loss: 0.4215\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4100 - val_loss: 0.4212\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4095 - val_loss: 0.4205\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4094 - val_loss: 0.4198\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4089 - val_loss: 0.4200\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4086 - val_loss: 0.4210\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4086 - val_loss: 0.4207\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4079 - val_loss: 0.4211\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.4216\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4073 - val_loss: 0.4196\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4072 - val_loss: 0.4208\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4217\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4071 - val_loss: 0.4218\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4061 - val_loss: 0.4221\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4064 - val_loss: 0.4215\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4061 - val_loss: 0.4242\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4059 - val_loss: 0.4216\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4055 - val_loss: 0.4215\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4050 - val_loss: 0.4229\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4052 - val_loss: 0.4220\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4221\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4046 - val_loss: 0.4225\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4232\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4045 - val_loss: 0.4223\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4035 - val_loss: 0.4235\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4243\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4036 - val_loss: 0.4235\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 3s 4ms/step - loss: 0.4384 - val_loss: 0.4020\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4308 - val_loss: 0.4006\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4283 - val_loss: 0.3979\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4265 - val_loss: 0.3971\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4254 - val_loss: 0.3968\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4245 - val_loss: 0.3968\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4240 - val_loss: 0.3969\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4230 - val_loss: 0.3975\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4228 - val_loss: 0.3970\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4222 - val_loss: 0.3972\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4217 - val_loss: 0.3969\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4208 - val_loss: 0.3975\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4206 - val_loss: 0.3980\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4203 - val_loss: 0.3977\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4196 - val_loss: 0.3981\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4191 - val_loss: 0.3977\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4188 - val_loss: 0.3982\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4185 - val_loss: 0.3985\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4181 - val_loss: 0.3988\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4177 - val_loss: 0.3989\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4174 - val_loss: 0.3978\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4168 - val_loss: 0.3989\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4167 - val_loss: 0.3978\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4167 - val_loss: 0.3982\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4158 - val_loss: 0.3993\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4158 - val_loss: 0.3994\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4154 - val_loss: 0.4004\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4155 - val_loss: 0.3982\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.3991\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4145 - val_loss: 0.3987\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4146 - val_loss: 0.3991\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.3995\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4140 - val_loss: 0.3981\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4138 - val_loss: 0.4003\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4134 - val_loss: 0.4010\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4134 - val_loss: 0.3999\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4130 - val_loss: 0.3998\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4128 - val_loss: 0.3989\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4127 - val_loss: 0.4021\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4126 - val_loss: 0.4005\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4122 - val_loss: 0.3995\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4123 - val_loss: 0.4008\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4119 - val_loss: 0.4011\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4115 - val_loss: 0.4003\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4116 - val_loss: 0.4027\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4110 - val_loss: 0.4012\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4116 - val_loss: 0.4018\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4107 - val_loss: 0.4023\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4113 - val_loss: 0.4005\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4107 - val_loss: 0.4013\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4289 - val_loss: 0.4437\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4199 - val_loss: 0.4421\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4173 - val_loss: 0.4411\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4159 - val_loss: 0.4414\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4149 - val_loss: 0.4408\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.4413\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4133 - val_loss: 0.4407\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4125 - val_loss: 0.4398\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4119 - val_loss: 0.4411\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4113 - val_loss: 0.4403\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4109 - val_loss: 0.4396\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4103 - val_loss: 0.4403\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4096 - val_loss: 0.4388\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4092 - val_loss: 0.4393\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 0.4390\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4391\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4078 - val_loss: 0.4395\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4077 - val_loss: 0.4385\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4372\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4380\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4064 - val_loss: 0.4397\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4384\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4060 - val_loss: 0.4392\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4058 - val_loss: 0.4384\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4051 - val_loss: 0.4396\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4054 - val_loss: 0.4396\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4398\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4046 - val_loss: 0.4405\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4043 - val_loss: 0.4413\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4042 - val_loss: 0.4401\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4415\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4040 - val_loss: 0.4407\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4031 - val_loss: 0.4413\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4034 - val_loss: 0.4418\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4033 - val_loss: 0.4409\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4027 - val_loss: 0.4422\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4026 - val_loss: 0.4419\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4026 - val_loss: 0.4420\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4023 - val_loss: 0.4420\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4019 - val_loss: 0.4424\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4016 - val_loss: 0.4435\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4013 - val_loss: 0.4426\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4014 - val_loss: 0.4427\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4012 - val_loss: 0.4445\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4007 - val_loss: 0.4445\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4009 - val_loss: 0.4428\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4009 - val_loss: 0.4437\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4007 - val_loss: 0.4439\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4002 - val_loss: 0.4440\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4000 - val_loss: 0.4451\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4385 - val_loss: 0.4317\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4272 - val_loss: 0.4301\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4238 - val_loss: 0.4291\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4221 - val_loss: 0.4282\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4208 - val_loss: 0.4271\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4192 - val_loss: 0.4274\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4181 - val_loss: 0.4272\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4176 - val_loss: 0.4268\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4171 - val_loss: 0.4273\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4162 - val_loss: 0.4271\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4156 - val_loss: 0.4272\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4152 - val_loss: 0.4271\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4144 - val_loss: 0.4274\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.4273\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4142 - val_loss: 0.4268\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4134 - val_loss: 0.4271\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4128 - val_loss: 0.4271\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4124 - val_loss: 0.4270\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4119 - val_loss: 0.4284\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4113 - val_loss: 0.4291\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4113 - val_loss: 0.4287\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4109 - val_loss: 0.4288\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4102 - val_loss: 0.4285\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4099 - val_loss: 0.4284\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4097 - val_loss: 0.4286\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4095 - val_loss: 0.4289\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4092 - val_loss: 0.4293\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4087 - val_loss: 0.4299\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4292\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4082 - val_loss: 0.4295\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4077 - val_loss: 0.4303\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4077 - val_loss: 0.4299\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4072 - val_loss: 0.4308\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4070 - val_loss: 0.4307\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4067 - val_loss: 0.4314\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4062 - val_loss: 0.4318\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4066 - val_loss: 0.4317\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4057 - val_loss: 0.4312\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4054 - val_loss: 0.4307\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4056 - val_loss: 0.4306\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4317\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4045 - val_loss: 0.4325\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4045 - val_loss: 0.4336\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4042 - val_loss: 0.4309\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4040 - val_loss: 0.4316\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4034 - val_loss: 0.4322\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4032 - val_loss: 0.4324\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4034 - val_loss: 0.4332\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4025 - val_loss: 0.4327\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4025 - val_loss: 0.4339\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4346 - val_loss: 0.4252\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4246 - val_loss: 0.4226\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4216 - val_loss: 0.4220\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4203 - val_loss: 0.4214\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4192 - val_loss: 0.4213\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4180 - val_loss: 0.4211\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4175 - val_loss: 0.4209\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4168 - val_loss: 0.4218\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4162 - val_loss: 0.4215\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4156 - val_loss: 0.4223\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4145 - val_loss: 0.4219\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4147 - val_loss: 0.4230\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4142 - val_loss: 0.4222\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4137 - val_loss: 0.4230\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4127 - val_loss: 0.4235\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4131 - val_loss: 0.4234\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4123 - val_loss: 0.4238\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4115 - val_loss: 0.4241\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4115 - val_loss: 0.4237\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4235\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4108 - val_loss: 0.4245\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4107 - val_loss: 0.4235\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4099 - val_loss: 0.4242\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4096 - val_loss: 0.4243\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4091 - val_loss: 0.4252\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4091 - val_loss: 0.4250\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4087 - val_loss: 0.4258\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4084 - val_loss: 0.4262\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4079 - val_loss: 0.4252\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4078 - val_loss: 0.4264\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4071 - val_loss: 0.4259\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4068 - val_loss: 0.4258\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4066 - val_loss: 0.4263\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4060 - val_loss: 0.4260\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4272\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4060 - val_loss: 0.4288\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4055 - val_loss: 0.4279\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4051 - val_loss: 0.4286\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4047 - val_loss: 0.4295\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4043 - val_loss: 0.4282\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4039 - val_loss: 0.4287\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4286\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4038 - val_loss: 0.4281\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4036 - val_loss: 0.4290\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4032 - val_loss: 0.4280\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4031 - val_loss: 0.4293\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4027 - val_loss: 0.4289\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4026 - val_loss: 0.4298\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4022 - val_loss: 0.4289\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4019 - val_loss: 0.4289\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4417 - val_loss: 0.4246\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4280 - val_loss: 0.4211\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4248 - val_loss: 0.4195\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4223 - val_loss: 0.4175\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4209 - val_loss: 0.4174\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4194 - val_loss: 0.4168\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4188 - val_loss: 0.4164\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4178 - val_loss: 0.4168\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4173 - val_loss: 0.4160\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4164 - val_loss: 0.4160\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4159 - val_loss: 0.4155\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4152 - val_loss: 0.4155\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4147 - val_loss: 0.4152\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4140 - val_loss: 0.4167\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4136 - val_loss: 0.4153\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4131 - val_loss: 0.4158\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4123 - val_loss: 0.4157\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4124 - val_loss: 0.4162\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4115 - val_loss: 0.4171\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4166\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4170\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4103 - val_loss: 0.4177\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4094 - val_loss: 0.4171\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4092 - val_loss: 0.4183\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4089 - val_loss: 0.4176\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4085 - val_loss: 0.4183\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4080 - val_loss: 0.4179\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.4180\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4186\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4071 - val_loss: 0.4185\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4066 - val_loss: 0.4193\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4186\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4056 - val_loss: 0.4197\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4054 - val_loss: 0.4189\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4056 - val_loss: 0.4193\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4053 - val_loss: 0.4186\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4049 - val_loss: 0.4201\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4043 - val_loss: 0.4201\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4042 - val_loss: 0.4194\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4039 - val_loss: 0.4199\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4033 - val_loss: 0.4206\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4038 - val_loss: 0.4198\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4208\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4030 - val_loss: 0.4202\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4023 - val_loss: 0.4194\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4025 - val_loss: 0.4192\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4019 - val_loss: 0.4199\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4018 - val_loss: 0.4200\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4009 - val_loss: 0.4200\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4014 - val_loss: 0.4209\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 3s 5ms/step - loss: 0.4263 - val_loss: 0.4484\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4179 - val_loss: 0.4470\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4155 - val_loss: 0.4470\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4141 - val_loss: 0.4465\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4132 - val_loss: 0.4471\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4123 - val_loss: 0.4460\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4114 - val_loss: 0.4468\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4110 - val_loss: 0.4464\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.4462\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4098 - val_loss: 0.4462\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4091 - val_loss: 0.4475\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4086 - val_loss: 0.4462\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4086 - val_loss: 0.4470\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4079 - val_loss: 0.4472\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4075 - val_loss: 0.4472\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4070 - val_loss: 0.4473\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4460\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4061 - val_loss: 0.4485\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4061 - val_loss: 0.4465\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4470\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4053 - val_loss: 0.4473\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4488\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4048 - val_loss: 0.4488\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4047 - val_loss: 0.4479\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4044 - val_loss: 0.4475\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4038 - val_loss: 0.4480\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4037 - val_loss: 0.4466\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4032 - val_loss: 0.4500\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4476\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4027 - val_loss: 0.4487\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4026 - val_loss: 0.4466\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4022 - val_loss: 0.4487\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4023 - val_loss: 0.4481\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4018 - val_loss: 0.4476\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4015 - val_loss: 0.4481\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4014 - val_loss: 0.4473\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4008 - val_loss: 0.4480\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4012 - val_loss: 0.4498\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4006 - val_loss: 0.4488\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4006 - val_loss: 0.4492\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4002 - val_loss: 0.4488\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4001 - val_loss: 0.4519\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4005 - val_loss: 0.4497\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3995 - val_loss: 0.4512\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3996 - val_loss: 0.4485\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3995 - val_loss: 0.4514\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3996 - val_loss: 0.4479\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3991 - val_loss: 0.4488\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3989 - val_loss: 0.4504\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3989 - val_loss: 0.4541\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "60\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4317 - val_loss: 0.4164\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4266 - val_loss: 0.4145\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4249 - val_loss: 0.4140\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4235 - val_loss: 0.4131\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4221 - val_loss: 0.4128\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4215 - val_loss: 0.4131\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4205 - val_loss: 0.4123\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4198 - val_loss: 0.4127\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4194 - val_loss: 0.4125\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4186 - val_loss: 0.4130\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4182 - val_loss: 0.4124\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4173 - val_loss: 0.4125\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4171 - val_loss: 0.4131\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4167 - val_loss: 0.4126\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4165 - val_loss: 0.4119\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4158 - val_loss: 0.4119\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4152 - val_loss: 0.4130\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4151 - val_loss: 0.4123\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4147 - val_loss: 0.4130\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4141 - val_loss: 0.4121\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4136 - val_loss: 0.4131\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4134 - val_loss: 0.4118\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4132 - val_loss: 0.4132\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4129 - val_loss: 0.4125\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4126 - val_loss: 0.4135\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4126 - val_loss: 0.4130\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4122 - val_loss: 0.4131\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4119 - val_loss: 0.4121\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4117 - val_loss: 0.4137\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4112 - val_loss: 0.4132\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4113 - val_loss: 0.4136\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4109 - val_loss: 0.4142\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4107 - val_loss: 0.4142\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.4135\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4104 - val_loss: 0.4140\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.4148\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4094 - val_loss: 0.4148\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4092 - val_loss: 0.4151\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 0.4172\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4086 - val_loss: 0.4143\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4080 - val_loss: 0.4150\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4142\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4080 - val_loss: 0.4146\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4073 - val_loss: 0.4147\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.4158\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4068 - val_loss: 0.4147\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4066 - val_loss: 0.4165\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4066 - val_loss: 0.4173\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4065 - val_loss: 0.4163\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4169\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4299 - val_loss: 0.4451\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4214 - val_loss: 0.4440\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4190 - val_loss: 0.4432\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4174 - val_loss: 0.4445\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4162 - val_loss: 0.4438\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4149 - val_loss: 0.4448\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4138 - val_loss: 0.4453\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4130 - val_loss: 0.4450\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4124 - val_loss: 0.4448\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4118 - val_loss: 0.4456\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4111 - val_loss: 0.4457\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4104 - val_loss: 0.4455\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.4465\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4095 - val_loss: 0.4457\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4087 - val_loss: 0.4461\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4468\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4082 - val_loss: 0.4460\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4076 - val_loss: 0.4463\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4073 - val_loss: 0.4473\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4065 - val_loss: 0.4477\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4062 - val_loss: 0.4476\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4057 - val_loss: 0.4482\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4055 - val_loss: 0.4481\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4050 - val_loss: 0.4482\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4046 - val_loss: 0.4486\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4043 - val_loss: 0.4487\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4502\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4506\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4033 - val_loss: 0.4490\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4031 - val_loss: 0.4516\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4025 - val_loss: 0.4513\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4029 - val_loss: 0.4514\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4021 - val_loss: 0.4556\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4022 - val_loss: 0.4529\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4014 - val_loss: 0.4522\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4014 - val_loss: 0.4500\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4011 - val_loss: 0.4529\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4008 - val_loss: 0.4513\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4004 - val_loss: 0.4508\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4000 - val_loss: 0.4512\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4004 - val_loss: 0.4527\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3995 - val_loss: 0.4549\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3996 - val_loss: 0.4520\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3995 - val_loss: 0.4526\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3988 - val_loss: 0.4527\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3991 - val_loss: 0.4532\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3985 - val_loss: 0.4513\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3985 - val_loss: 0.4542\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3982 - val_loss: 0.4537\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3984 - val_loss: 0.4538\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4293 - val_loss: 0.4425\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4203 - val_loss: 0.4397\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4176 - val_loss: 0.4392\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4159 - val_loss: 0.4384\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4147 - val_loss: 0.4387\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4138 - val_loss: 0.4387\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.4127 - val_loss: 0.4387\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4122 - val_loss: 0.4389\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4115 - val_loss: 0.4383\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4106 - val_loss: 0.4392\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4102 - val_loss: 0.4394\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4388\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4092 - val_loss: 0.4387\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4086 - val_loss: 0.4386\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4393\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4079 - val_loss: 0.4382\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4073 - val_loss: 0.4390\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4391\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4067 - val_loss: 0.4398\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4061 - val_loss: 0.4397\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4059 - val_loss: 0.4394\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4052 - val_loss: 0.4400\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4048 - val_loss: 0.4396\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4042 - val_loss: 0.4400\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4043 - val_loss: 0.4407\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4039 - val_loss: 0.4401\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4415\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4407\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4031 - val_loss: 0.4418\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4028 - val_loss: 0.4408\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4024 - val_loss: 0.4402\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4018 - val_loss: 0.4408\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4018 - val_loss: 0.4409\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4015 - val_loss: 0.4410\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4011 - val_loss: 0.4421\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4007 - val_loss: 0.4410\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4005 - val_loss: 0.4423\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4002 - val_loss: 0.4423\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4001 - val_loss: 0.4423\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4000 - val_loss: 0.4422\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3992 - val_loss: 0.4423\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3992 - val_loss: 0.4422\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3994 - val_loss: 0.4430\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3986 - val_loss: 0.4441\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3985 - val_loss: 0.4435\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3985 - val_loss: 0.4434\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3980 - val_loss: 0.4427\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3980 - val_loss: 0.4439\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3977 - val_loss: 0.4436\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3978 - val_loss: 0.4445\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 3s 4ms/step - loss: 0.4372 - val_loss: 0.4305\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4257 - val_loss: 0.4286\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4231 - val_loss: 0.4265\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4217 - val_loss: 0.4261\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4202 - val_loss: 0.4271\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4193 - val_loss: 0.4262\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4181 - val_loss: 0.4259\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4172 - val_loss: 0.4264\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4160 - val_loss: 0.4257\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4155 - val_loss: 0.4269\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.4141 - val_loss: 0.4259\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4137 - val_loss: 0.4277\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4134 - val_loss: 0.4270\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4124 - val_loss: 0.4268\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4117 - val_loss: 0.4278\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4113 - val_loss: 0.4276\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4110 - val_loss: 0.4274\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4281\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4101 - val_loss: 0.4273\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4093 - val_loss: 0.4286\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4090 - val_loss: 0.4279\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4081 - val_loss: 0.4267\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4077 - val_loss: 0.4288\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4073 - val_loss: 0.4295\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4074 - val_loss: 0.4285\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4067 - val_loss: 0.4285\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4294\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4065 - val_loss: 0.4290\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4059 - val_loss: 0.4300\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4055 - val_loss: 0.4296\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4057 - val_loss: 0.4306\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4047 - val_loss: 0.4300\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4045 - val_loss: 0.4299\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4305\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4343\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4036 - val_loss: 0.4320\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4032 - val_loss: 0.4312\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4031 - val_loss: 0.4308\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4030 - val_loss: 0.4317\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4023 - val_loss: 0.4316\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4021 - val_loss: 0.4321\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4019 - val_loss: 0.4343\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4020 - val_loss: 0.4314\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4016 - val_loss: 0.4330\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4010 - val_loss: 0.4341\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4013 - val_loss: 0.4341\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4006 - val_loss: 0.4350\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4005 - val_loss: 0.4347\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4002 - val_loss: 0.4359\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4000 - val_loss: 0.4348\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4307 - val_loss: 0.4398\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4222 - val_loss: 0.4377\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4190 - val_loss: 0.4361\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4179 - val_loss: 0.4343\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4157 - val_loss: 0.4342\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4156 - val_loss: 0.4327\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4146 - val_loss: 0.4329\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4136 - val_loss: 0.4327\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4124 - val_loss: 0.4336\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4120 - val_loss: 0.4340\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4114 - val_loss: 0.4335\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4331\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4102 - val_loss: 0.4336\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4098 - val_loss: 0.4338\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4094 - val_loss: 0.4345\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4090 - val_loss: 0.4342\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4342\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4080 - val_loss: 0.4353\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4079 - val_loss: 0.4355\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.4356\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4070 - val_loss: 0.4350\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4357\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4065 - val_loss: 0.4367\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4064 - val_loss: 0.4355\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4053 - val_loss: 0.4369\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4053 - val_loss: 0.4359\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4054 - val_loss: 0.4369\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4048 - val_loss: 0.4361\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4374\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4045 - val_loss: 0.4372\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4376\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4038 - val_loss: 0.4385\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4037 - val_loss: 0.4367\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4032 - val_loss: 0.4375\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4029 - val_loss: 0.4386\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4026 - val_loss: 0.4384\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4028 - val_loss: 0.4376\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4024 - val_loss: 0.4370\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4018 - val_loss: 0.4400\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4016 - val_loss: 0.4390\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4010 - val_loss: 0.4387\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4014 - val_loss: 0.4389\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4004 - val_loss: 0.4392\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4011 - val_loss: 0.4410\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4008 - val_loss: 0.4394\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4004 - val_loss: 0.4386\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3998 - val_loss: 0.4413\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4002 - val_loss: 0.4392\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3993 - val_loss: 0.4408\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3991 - val_loss: 0.4406\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4291 - val_loss: 0.4331\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4222 - val_loss: 0.4317\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4199 - val_loss: 0.4304\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4183 - val_loss: 0.4300\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4175 - val_loss: 0.4307\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4167 - val_loss: 0.4303\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4158 - val_loss: 0.4299\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4151 - val_loss: 0.4299\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.4289\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4138 - val_loss: 0.4290\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4134 - val_loss: 0.4296\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4127 - val_loss: 0.4294\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4125 - val_loss: 0.4287\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4117 - val_loss: 0.4289\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4113 - val_loss: 0.4311\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4107 - val_loss: 0.4295\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4106 - val_loss: 0.4298\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4100 - val_loss: 0.4304\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4304\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.4312\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4094 - val_loss: 0.4314\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4092 - val_loss: 0.4300\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4310\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4307\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4081 - val_loss: 0.4313\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4079 - val_loss: 0.4321\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4072 - val_loss: 0.4309\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4072 - val_loss: 0.4330\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4320\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4068 - val_loss: 0.4328\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4064 - val_loss: 0.4317\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4060 - val_loss: 0.4326\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4061 - val_loss: 0.4326\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4061 - val_loss: 0.4330\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4059 - val_loss: 0.4325\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4052 - val_loss: 0.4336\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4054 - val_loss: 0.4333\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4332\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4044 - val_loss: 0.4344\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4044 - val_loss: 0.4333\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4038 - val_loss: 0.4364\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4328\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4349\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4029 - val_loss: 0.4340\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4030 - val_loss: 0.4357\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4030 - val_loss: 0.4374\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4024 - val_loss: 0.4340\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4028 - val_loss: 0.4359\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4018 - val_loss: 0.4344\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4024 - val_loss: 0.4371\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4314 - val_loss: 0.4530\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4181 - val_loss: 0.4513\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4154 - val_loss: 0.4509\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4133 - val_loss: 0.4508\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4125 - val_loss: 0.4497\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4116 - val_loss: 0.4508\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4111 - val_loss: 0.4496\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4488\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4095 - val_loss: 0.4487\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4092 - val_loss: 0.4496\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4089 - val_loss: 0.4489\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4081 - val_loss: 0.4490\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.4483\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4068 - val_loss: 0.4492\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4484\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4061 - val_loss: 0.4486\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4053 - val_loss: 0.4490\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4055 - val_loss: 0.4489\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4050 - val_loss: 0.4487\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4047 - val_loss: 0.4499\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4046 - val_loss: 0.4488\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4040 - val_loss: 0.4494\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4038 - val_loss: 0.4496\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4033 - val_loss: 0.4494\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4036 - val_loss: 0.4493\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4499\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4027 - val_loss: 0.4496\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4024 - val_loss: 0.4500\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4019 - val_loss: 0.4506\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4022 - val_loss: 0.4490\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4014 - val_loss: 0.4497\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4015 - val_loss: 0.4517\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4014 - val_loss: 0.4507\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4009 - val_loss: 0.4497\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4005 - val_loss: 0.4512\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4009 - val_loss: 0.4507\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4004 - val_loss: 0.4507\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4001 - val_loss: 0.4505\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3998 - val_loss: 0.4515\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3994 - val_loss: 0.4501\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3995 - val_loss: 0.4515\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3993 - val_loss: 0.4508\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3991 - val_loss: 0.4509\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3989 - val_loss: 0.4516\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3983 - val_loss: 0.4524\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3987 - val_loss: 0.4522\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3983 - val_loss: 0.4534\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3983 - val_loss: 0.4522\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3978 - val_loss: 0.4529\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3975 - val_loss: 0.4528\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4412 - val_loss: 0.4015\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4312 - val_loss: 0.3998\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4288 - val_loss: 0.3995\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4271 - val_loss: 0.3992\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4258 - val_loss: 0.3995\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4247 - val_loss: 0.3994\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4234 - val_loss: 0.3998\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4228 - val_loss: 0.4001\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4218 - val_loss: 0.3999\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4218 - val_loss: 0.3997\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4205 - val_loss: 0.3997\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4205 - val_loss: 0.4003\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4195 - val_loss: 0.4005\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4194 - val_loss: 0.4021\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4188 - val_loss: 0.4003\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4184 - val_loss: 0.4006\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4176 - val_loss: 0.4006\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4173 - val_loss: 0.4021\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4168 - val_loss: 0.4020\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4165 - val_loss: 0.4032\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4161 - val_loss: 0.4034\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4159 - val_loss: 0.4036\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4159 - val_loss: 0.4050\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.4039\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4148 - val_loss: 0.4049\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4146 - val_loss: 0.4051\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4141 - val_loss: 0.4045\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4138 - val_loss: 0.4054\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4130 - val_loss: 0.4051\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4132 - val_loss: 0.4044\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4130 - val_loss: 0.4047\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4126 - val_loss: 0.4051\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4123 - val_loss: 0.4052\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4122 - val_loss: 0.4048\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4120 - val_loss: 0.4049\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4117 - val_loss: 0.4052\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4115 - val_loss: 0.4070\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4112 - val_loss: 0.4062\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4112 - val_loss: 0.4053\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4106 - val_loss: 0.4063\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4075\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4069\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4085\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.4065\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4094 - val_loss: 0.4073\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4090 - val_loss: 0.4092\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4092 - val_loss: 0.4083\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4086 - val_loss: 0.4093\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4103\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4079 - val_loss: 0.4089\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4307 - val_loss: 0.4380\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4218 - val_loss: 0.4355\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4190 - val_loss: 0.4343\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4178 - val_loss: 0.4333\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4165 - val_loss: 0.4320\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4155 - val_loss: 0.4319\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4146 - val_loss: 0.4317\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4142 - val_loss: 0.4322\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4137 - val_loss: 0.4317\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4134 - val_loss: 0.4327\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4129 - val_loss: 0.4315\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4123 - val_loss: 0.4326\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4122 - val_loss: 0.4314\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4116 - val_loss: 0.4314\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4111 - val_loss: 0.4343\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4319\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4319\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4104 - val_loss: 0.4331\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4101 - val_loss: 0.4325\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.4329\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4091 - val_loss: 0.4340\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4089 - val_loss: 0.4331\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4086 - val_loss: 0.4332\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4348\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4082 - val_loss: 0.4338\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4076 - val_loss: 0.4340\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4334\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4350\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4353\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4350\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4063 - val_loss: 0.4342\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4058 - val_loss: 0.4336\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4058 - val_loss: 0.4374\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4056 - val_loss: 0.4352\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4051 - val_loss: 0.4351\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4048 - val_loss: 0.4367\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4048 - val_loss: 0.4372\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4045 - val_loss: 0.4365\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4040 - val_loss: 0.4354\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4367\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4036 - val_loss: 0.4377\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4032 - val_loss: 0.4402\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4031 - val_loss: 0.4394\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4032 - val_loss: 0.4399\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4025 - val_loss: 0.4384\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4027 - val_loss: 0.4394\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4024 - val_loss: 0.4394\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4022 - val_loss: 0.4403\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4020 - val_loss: 0.4395\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4016 - val_loss: 0.4419\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4370 - val_loss: 0.4085\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4284 - val_loss: 0.4086\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4262 - val_loss: 0.4077\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4243 - val_loss: 0.4079\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4231 - val_loss: 0.4075\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4223 - val_loss: 0.4065\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4211 - val_loss: 0.4065\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4204 - val_loss: 0.4063\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4196 - val_loss: 0.4064\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4193 - val_loss: 0.4055\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4184 - val_loss: 0.4047\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4179 - val_loss: 0.4066\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4176 - val_loss: 0.4058\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4167 - val_loss: 0.4044\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4166 - val_loss: 0.4065\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4159 - val_loss: 0.4060\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4158 - val_loss: 0.4070\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4150 - val_loss: 0.4054\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4146 - val_loss: 0.4056\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4145 - val_loss: 0.4065\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4140 - val_loss: 0.4078\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4137 - val_loss: 0.4061\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4130 - val_loss: 0.4069\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4129 - val_loss: 0.4060\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4124 - val_loss: 0.4069\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4118 - val_loss: 0.4082\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4116 - val_loss: 0.4085\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4116 - val_loss: 0.4074\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4107 - val_loss: 0.4059\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4091\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4081\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4101 - val_loss: 0.4063\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4101 - val_loss: 0.4078\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.4079\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4092 - val_loss: 0.4074\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4091 - val_loss: 0.4082\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4089 - val_loss: 0.4090\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4091 - val_loss: 0.4072\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4087\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4081 - val_loss: 0.4119\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4081 - val_loss: 0.4100\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4078 - val_loss: 0.4094\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4072 - val_loss: 0.4084\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4077 - val_loss: 0.4103\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4085\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4073 - val_loss: 0.4117\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4066 - val_loss: 0.4099\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4070 - val_loss: 0.4099\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4061 - val_loss: 0.4121\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4060 - val_loss: 0.4110\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "70\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4373 - val_loss: 0.4064\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4293 - val_loss: 0.4044\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4265 - val_loss: 0.4034\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4248 - val_loss: 0.4025\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4233 - val_loss: 0.4035\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4224 - val_loss: 0.4041\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4217 - val_loss: 0.4045\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4211 - val_loss: 0.4041\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4203 - val_loss: 0.4042\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4197 - val_loss: 0.4045\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4191 - val_loss: 0.4041\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4185 - val_loss: 0.4057\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4181 - val_loss: 0.4044\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4176 - val_loss: 0.4051\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4172 - val_loss: 0.4063\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4168 - val_loss: 0.4060\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4162 - val_loss: 0.4053\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4159 - val_loss: 0.4060\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4154 - val_loss: 0.4061\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4149 - val_loss: 0.4085\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4144 - val_loss: 0.4072\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4140 - val_loss: 0.4081\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4136 - val_loss: 0.4073\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4136 - val_loss: 0.4078\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4130 - val_loss: 0.4088\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4128 - val_loss: 0.4100\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4123 - val_loss: 0.4089\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4119 - val_loss: 0.4092\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4117 - val_loss: 0.4097\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4113 - val_loss: 0.4098\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4111 - val_loss: 0.4105\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4108 - val_loss: 0.4106\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4106 - val_loss: 0.4103\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4107 - val_loss: 0.4112\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4101 - val_loss: 0.4130\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4100 - val_loss: 0.4109\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4094 - val_loss: 0.4115\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4092 - val_loss: 0.4109\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4091 - val_loss: 0.4137\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4090 - val_loss: 0.4121\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 0.4138\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4086 - val_loss: 0.4117\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4145\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4087 - val_loss: 0.4150\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4080 - val_loss: 0.4150\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4076 - val_loss: 0.4142\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4076 - val_loss: 0.4136\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4078 - val_loss: 0.4134\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4070 - val_loss: 0.4149\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4067 - val_loss: 0.4142\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4216 - val_loss: 0.4692\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.4672\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4121 - val_loss: 0.4674\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4107 - val_loss: 0.4669\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4092 - val_loss: 0.4675\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4663\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.4663\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4067 - val_loss: 0.4655\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4664\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4054 - val_loss: 0.4675\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4047 - val_loss: 0.4655\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4042 - val_loss: 0.4660\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4036 - val_loss: 0.4657\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4034 - val_loss: 0.4672\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4026 - val_loss: 0.4664\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4024 - val_loss: 0.4668\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4020 - val_loss: 0.4671\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4013 - val_loss: 0.4677\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4012 - val_loss: 0.4679\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4007 - val_loss: 0.4680\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4002 - val_loss: 0.4677\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3998 - val_loss: 0.4689\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3996 - val_loss: 0.4707\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3998 - val_loss: 0.4701\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3993 - val_loss: 0.4703\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3989 - val_loss: 0.4691\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3987 - val_loss: 0.4700\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3983 - val_loss: 0.4723\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3981 - val_loss: 0.4712\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3981 - val_loss: 0.4710\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3976 - val_loss: 0.4696\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3974 - val_loss: 0.4711\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3970 - val_loss: 0.4717\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3969 - val_loss: 0.4700\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3967 - val_loss: 0.4718\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3966 - val_loss: 0.4709\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3966 - val_loss: 0.4706\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3962 - val_loss: 0.4708\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3958 - val_loss: 0.4728\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3960 - val_loss: 0.4732\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3953 - val_loss: 0.4714\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3955 - val_loss: 0.4722\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3949 - val_loss: 0.4753\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3950 - val_loss: 0.4750\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3948 - val_loss: 0.4760\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3947 - val_loss: 0.4701\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3944 - val_loss: 0.4757\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3937 - val_loss: 0.4738\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3939 - val_loss: 0.4736\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3939 - val_loss: 0.4726\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4446 - val_loss: 0.3962\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4332 - val_loss: 0.3937\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4297 - val_loss: 0.3929\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4279 - val_loss: 0.3929\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4264 - val_loss: 0.3925\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4253 - val_loss: 0.3926\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4247 - val_loss: 0.3917\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4236 - val_loss: 0.3914\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4229 - val_loss: 0.3920\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4225 - val_loss: 0.3910\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4216 - val_loss: 0.3919\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4213 - val_loss: 0.3910\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4204 - val_loss: 0.3932\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4204 - val_loss: 0.3918\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4197 - val_loss: 0.3931\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4194 - val_loss: 0.3925\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4188 - val_loss: 0.3932\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4184 - val_loss: 0.3932\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4181 - val_loss: 0.3932\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4177 - val_loss: 0.3938\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4175 - val_loss: 0.3937\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4169 - val_loss: 0.3939\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4164 - val_loss: 0.3943\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4163 - val_loss: 0.3949\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4157 - val_loss: 0.3940\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.3942\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4150 - val_loss: 0.3943\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4148 - val_loss: 0.3946\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4146 - val_loss: 0.3952\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4143 - val_loss: 0.3962\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4138 - val_loss: 0.3942\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4133 - val_loss: 0.3958\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4129 - val_loss: 0.3953\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4129 - val_loss: 0.3963\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4123 - val_loss: 0.3961\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4119 - val_loss: 0.3974\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4121 - val_loss: 0.3961\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4113 - val_loss: 0.3978\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4114 - val_loss: 0.3992\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4110 - val_loss: 0.3974\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.3983\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4102 - val_loss: 0.3987\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.3984\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4100 - val_loss: 0.3982\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4086 - val_loss: 0.3997\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4092 - val_loss: 0.3996\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4089 - val_loss: 0.3998\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4087 - val_loss: 0.4013\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4084 - val_loss: 0.4000\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4086 - val_loss: 0.4003\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4377 - val_loss: 0.4209\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4289 - val_loss: 0.4196\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4263 - val_loss: 0.4173\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4239 - val_loss: 0.4167\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4219 - val_loss: 0.4152\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4211 - val_loss: 0.4157\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4201 - val_loss: 0.4151\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4191 - val_loss: 0.4149\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4183 - val_loss: 0.4146\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4176 - val_loss: 0.4155\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4170 - val_loss: 0.4151\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4164 - val_loss: 0.4140\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4158 - val_loss: 0.4138\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4154 - val_loss: 0.4140\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4148 - val_loss: 0.4132\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4143 - val_loss: 0.4145\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4137 - val_loss: 0.4136\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4132 - val_loss: 0.4166\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4135 - val_loss: 0.4141\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4126 - val_loss: 0.4163\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4120 - val_loss: 0.4148\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4123 - val_loss: 0.4141\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4115 - val_loss: 0.4138\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4112 - val_loss: 0.4143\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4144\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.4172\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4101 - val_loss: 0.4144\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4097 - val_loss: 0.4159\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4100 - val_loss: 0.4160\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4092 - val_loss: 0.4164\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4095 - val_loss: 0.4159\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 0.4189\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4087 - val_loss: 0.4168\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4080 - val_loss: 0.4152\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4084 - val_loss: 0.4176\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4080 - val_loss: 0.4171\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4196\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4188\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4066 - val_loss: 0.4168\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4066 - val_loss: 0.4164\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4062 - val_loss: 0.4163\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4060 - val_loss: 0.4207\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4057 - val_loss: 0.4174\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4056 - val_loss: 0.4182\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4053 - val_loss: 0.4213\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4047 - val_loss: 0.4188\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4050 - val_loss: 0.4181\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4045 - val_loss: 0.4155\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4044 - val_loss: 0.4187\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4041 - val_loss: 0.4180\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4364 - val_loss: 0.4128\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4285 - val_loss: 0.4096\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4261 - val_loss: 0.4086\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4246 - val_loss: 0.4075\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4236 - val_loss: 0.4080\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4228 - val_loss: 0.4084\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4222 - val_loss: 0.4073\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4214 - val_loss: 0.4066\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4212 - val_loss: 0.4064\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4205 - val_loss: 0.4061\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4201 - val_loss: 0.4064\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4198 - val_loss: 0.4063\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4194 - val_loss: 0.4055\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4187 - val_loss: 0.4082\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4182 - val_loss: 0.4068\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4176 - val_loss: 0.4072\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4176 - val_loss: 0.4069\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4171 - val_loss: 0.4069\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4164 - val_loss: 0.4058\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4162 - val_loss: 0.4062\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4157 - val_loss: 0.4066\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.4064\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4151 - val_loss: 0.4067\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4146 - val_loss: 0.4057\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4142 - val_loss: 0.4069\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4141 - val_loss: 0.4057\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4141 - val_loss: 0.4068\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4137 - val_loss: 0.4063\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4131 - val_loss: 0.4064\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4133 - val_loss: 0.4073\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4125 - val_loss: 0.4074\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4125 - val_loss: 0.4062\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4123 - val_loss: 0.4062\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4119 - val_loss: 0.4069\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4119 - val_loss: 0.4078\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4115 - val_loss: 0.4074\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4115 - val_loss: 0.4081\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4108 - val_loss: 0.4076\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4106 - val_loss: 0.4080\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4106 - val_loss: 0.4085\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4104 - val_loss: 0.4079\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4102 - val_loss: 0.4080\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4101 - val_loss: 0.4076\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4095 - val_loss: 0.4075\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4095 - val_loss: 0.4078\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4095 - val_loss: 0.4084\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4094 - val_loss: 0.4064\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4086\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 0.4079\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4089 - val_loss: 0.4082\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 3s 4ms/step - loss: 0.4360 - val_loss: 0.4163\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4271 - val_loss: 0.4136\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4247 - val_loss: 0.4111\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4234 - val_loss: 0.4125\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4225 - val_loss: 0.4106\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4217 - val_loss: 0.4108\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4210 - val_loss: 0.4103\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4204 - val_loss: 0.4092\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4198 - val_loss: 0.4087\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4191 - val_loss: 0.4097\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4182 - val_loss: 0.4098\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4177 - val_loss: 0.4094\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4173 - val_loss: 0.4086\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4168 - val_loss: 0.4099\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4160 - val_loss: 0.4087\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4160 - val_loss: 0.4087\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4151 - val_loss: 0.4111\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4149 - val_loss: 0.4090\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4145 - val_loss: 0.4096\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4143 - val_loss: 0.4091\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4136 - val_loss: 0.4105\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4135 - val_loss: 0.4098\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4131 - val_loss: 0.4112\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4127 - val_loss: 0.4087\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4121 - val_loss: 0.4090\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4120 - val_loss: 0.4093\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4117 - val_loss: 0.4094\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4114 - val_loss: 0.4090\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4110 - val_loss: 0.4111\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4107 - val_loss: 0.4102\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4098\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4099 - val_loss: 0.4102\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4102 - val_loss: 0.4100\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.4129\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4093 - val_loss: 0.4105\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4091 - val_loss: 0.4097\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4103\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4080 - val_loss: 0.4117\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4078 - val_loss: 0.4118\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4080 - val_loss: 0.4118\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4079 - val_loss: 0.4096\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4072 - val_loss: 0.4122\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4066 - val_loss: 0.4105\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4067 - val_loss: 0.4112\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4059 - val_loss: 0.4126\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4060 - val_loss: 0.4116\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4056 - val_loss: 0.4117\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4059 - val_loss: 0.4130\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4052 - val_loss: 0.4109\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4046 - val_loss: 0.4123\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4368 - val_loss: 0.4266\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4240 - val_loss: 0.4247\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4215 - val_loss: 0.4228\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4204 - val_loss: 0.4229\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4196 - val_loss: 0.4233\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4190 - val_loss: 0.4226\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4183 - val_loss: 0.4227\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4175 - val_loss: 0.4219\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4173 - val_loss: 0.4226\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4165 - val_loss: 0.4231\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4162 - val_loss: 0.4224\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4157 - val_loss: 0.4218\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4154 - val_loss: 0.4214\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4150 - val_loss: 0.4220\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4145 - val_loss: 0.4220\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4140 - val_loss: 0.4217\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4132 - val_loss: 0.4232\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4129 - val_loss: 0.4230\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4125 - val_loss: 0.4211\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4120 - val_loss: 0.4207\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4116 - val_loss: 0.4219\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4227\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4218\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4221\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.4227\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4098 - val_loss: 0.4224\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4218\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4091 - val_loss: 0.4223\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4087 - val_loss: 0.4216\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4077 - val_loss: 0.4241\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4081 - val_loss: 0.4233\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4077 - val_loss: 0.4230\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4073 - val_loss: 0.4225\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4070 - val_loss: 0.4227\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4244\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4068 - val_loss: 0.4227\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4229\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4060 - val_loss: 0.4232\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4055 - val_loss: 0.4235\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4059 - val_loss: 0.4243\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4055 - val_loss: 0.4247\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4235\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4049 - val_loss: 0.4255\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4049 - val_loss: 0.4271\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4043 - val_loss: 0.4264\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4046 - val_loss: 0.4267\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4039 - val_loss: 0.4267\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4040 - val_loss: 0.4261\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4263\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4033 - val_loss: 0.4257\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4356 - val_loss: 0.4244\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4253 - val_loss: 0.4220\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4225 - val_loss: 0.4218\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4208 - val_loss: 0.4213\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4201 - val_loss: 0.4209\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4187 - val_loss: 0.4201\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4182 - val_loss: 0.4205\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4176 - val_loss: 0.4205\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4172 - val_loss: 0.4201\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4165 - val_loss: 0.4211\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4162 - val_loss: 0.4202\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4156 - val_loss: 0.4205\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.4199\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4150 - val_loss: 0.4205\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4140 - val_loss: 0.4199\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4136 - val_loss: 0.4208\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4134 - val_loss: 0.4209\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4132 - val_loss: 0.4209\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4125 - val_loss: 0.4214\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4121 - val_loss: 0.4212\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4118 - val_loss: 0.4208\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4113 - val_loss: 0.4212\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4230\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.4235\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4102 - val_loss: 0.4226\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.4231\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4092 - val_loss: 0.4225\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4090 - val_loss: 0.4229\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4089 - val_loss: 0.4235\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4079 - val_loss: 0.4234\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4079 - val_loss: 0.4237\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4075 - val_loss: 0.4255\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4068 - val_loss: 0.4257\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4072 - val_loss: 0.4258\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4066 - val_loss: 0.4263\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4066 - val_loss: 0.4262\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4266\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4056 - val_loss: 0.4284\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4053 - val_loss: 0.4289\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4050 - val_loss: 0.4267\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4046 - val_loss: 0.4297\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4044 - val_loss: 0.4292\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4036 - val_loss: 0.4291\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4038 - val_loss: 0.4318\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4031 - val_loss: 0.4297\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4293\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4026 - val_loss: 0.4319\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4024 - val_loss: 0.4348\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4021 - val_loss: 0.4336\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4021 - val_loss: 0.4344\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4341 - val_loss: 0.4221\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4245 - val_loss: 0.4200\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4223 - val_loss: 0.4197\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4209 - val_loss: 0.4199\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4198 - val_loss: 0.4194\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4187 - val_loss: 0.4217\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4179 - val_loss: 0.4215\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4171 - val_loss: 0.4221\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4167 - val_loss: 0.4208\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4160 - val_loss: 0.4197\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4154 - val_loss: 0.4206\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4149 - val_loss: 0.4209\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.4211\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4137 - val_loss: 0.4232\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4134 - val_loss: 0.4200\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4130 - val_loss: 0.4204\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4124 - val_loss: 0.4201\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4122 - val_loss: 0.4210\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4117 - val_loss: 0.4203\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4112 - val_loss: 0.4202\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4107 - val_loss: 0.4216\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4107 - val_loss: 0.4215\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.4204\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4098 - val_loss: 0.4221\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.4215\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4090 - val_loss: 0.4201\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4089 - val_loss: 0.4216\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4226\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4081 - val_loss: 0.4209\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4081 - val_loss: 0.4208\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4074 - val_loss: 0.4234\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4076 - val_loss: 0.4226\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4070 - val_loss: 0.4206\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4068 - val_loss: 0.4213\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4229\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4067 - val_loss: 0.4218\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4062 - val_loss: 0.4217\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4060 - val_loss: 0.4204\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4059 - val_loss: 0.4204\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4054 - val_loss: 0.4216\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4053 - val_loss: 0.4228\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4053 - val_loss: 0.4207\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4050 - val_loss: 0.4221\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4048 - val_loss: 0.4211\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4049 - val_loss: 0.4229\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4042 - val_loss: 0.4200\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4042 - val_loss: 0.4213\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4040 - val_loss: 0.4196\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4244\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4232\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 3s 5ms/step - loss: 0.4270 - val_loss: 0.4393\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4206 - val_loss: 0.4384\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4186 - val_loss: 0.4367\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4170 - val_loss: 0.4359\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4163 - val_loss: 0.4351\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.4350\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4147 - val_loss: 0.4341\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4141 - val_loss: 0.4346\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4136 - val_loss: 0.4341\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4129 - val_loss: 0.4344\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4124 - val_loss: 0.4347\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4120 - val_loss: 0.4343\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4113 - val_loss: 0.4345\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4105 - val_loss: 0.4362\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4358\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4101 - val_loss: 0.4353\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.4358\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 0.4356\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4355\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4079 - val_loss: 0.4357\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4077 - val_loss: 0.4369\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4070 - val_loss: 0.4375\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4068 - val_loss: 0.4373\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4380\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4057 - val_loss: 0.4382\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4056 - val_loss: 0.4376\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4380\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4047 - val_loss: 0.4384\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4042 - val_loss: 0.4381\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4394\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4036 - val_loss: 0.4394\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4032 - val_loss: 0.4396\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4023 - val_loss: 0.4404\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4024 - val_loss: 0.4409\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4019 - val_loss: 0.4412\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4018 - val_loss: 0.4412\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4016 - val_loss: 0.4418\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4006 - val_loss: 0.4423\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4007 - val_loss: 0.4429\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4007 - val_loss: 0.4428\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4001 - val_loss: 0.4426\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3999 - val_loss: 0.4420\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3990 - val_loss: 0.4458\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3992 - val_loss: 0.4433\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3986 - val_loss: 0.4455\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3984 - val_loss: 0.4482\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3985 - val_loss: 0.4455\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3980 - val_loss: 0.4442\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3972 - val_loss: 0.4462\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3978 - val_loss: 0.4460\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "80\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4195 - val_loss: 0.4641\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4131 - val_loss: 0.4641\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4110 - val_loss: 0.4636\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4098 - val_loss: 0.4641\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4086 - val_loss: 0.4633\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.4640\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4071 - val_loss: 0.4658\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4062 - val_loss: 0.4639\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4056 - val_loss: 0.4656\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4050 - val_loss: 0.4645\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4044 - val_loss: 0.4650\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4042 - val_loss: 0.4654\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4034 - val_loss: 0.4652\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4028 - val_loss: 0.4665\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4026 - val_loss: 0.4664\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4021 - val_loss: 0.4664\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4020 - val_loss: 0.4662\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4012 - val_loss: 0.4673\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4012 - val_loss: 0.4676\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4008 - val_loss: 0.4677\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4002 - val_loss: 0.4688\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4003 - val_loss: 0.4678\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3997 - val_loss: 0.4695\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3993 - val_loss: 0.4694\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3988 - val_loss: 0.4699\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3988 - val_loss: 0.4708\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3984 - val_loss: 0.4693\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3986 - val_loss: 0.4692\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3980 - val_loss: 0.4699\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3979 - val_loss: 0.4711\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3974 - val_loss: 0.4708\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3974 - val_loss: 0.4706\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3973 - val_loss: 0.4722\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3969 - val_loss: 0.4718\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3966 - val_loss: 0.4727\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3961 - val_loss: 0.4724\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3961 - val_loss: 0.4740\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3960 - val_loss: 0.4723\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3956 - val_loss: 0.4727\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3957 - val_loss: 0.4739\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3957 - val_loss: 0.4742\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3951 - val_loss: 0.4734\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3952 - val_loss: 0.4740\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3946 - val_loss: 0.4750\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3947 - val_loss: 0.4751\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3945 - val_loss: 0.4766\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3946 - val_loss: 0.4763\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3942 - val_loss: 0.4747\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3935 - val_loss: 0.4755\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3936 - val_loss: 0.4748\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4367 - val_loss: 0.4257\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4250 - val_loss: 0.4232\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4223 - val_loss: 0.4226\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4203 - val_loss: 0.4227\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4194 - val_loss: 0.4217\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4189 - val_loss: 0.4221\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4182 - val_loss: 0.4216\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4176 - val_loss: 0.4217\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4171 - val_loss: 0.4224\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4167 - val_loss: 0.4218\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4161 - val_loss: 0.4220\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4158 - val_loss: 0.4222\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4153 - val_loss: 0.4223\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4150 - val_loss: 0.4230\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4141 - val_loss: 0.4228\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4139 - val_loss: 0.4229\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4134 - val_loss: 0.4224\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4130 - val_loss: 0.4218\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4124 - val_loss: 0.4208\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4122 - val_loss: 0.4210\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4117 - val_loss: 0.4213\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4112 - val_loss: 0.4244\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4109 - val_loss: 0.4218\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4104 - val_loss: 0.4235\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4097 - val_loss: 0.4241\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4095 - val_loss: 0.4234\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4089 - val_loss: 0.4243\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 0.4232\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4082 - val_loss: 0.4229\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4081 - val_loss: 0.4223\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.4261\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4238\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4071 - val_loss: 0.4226\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4065 - val_loss: 0.4259\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4069 - val_loss: 0.4249\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4063 - val_loss: 0.4253\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4058 - val_loss: 0.4243\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4061 - val_loss: 0.4256\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4057 - val_loss: 0.4249\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4051 - val_loss: 0.4251\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4053 - val_loss: 0.4274\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4050 - val_loss: 0.4265\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4050 - val_loss: 0.4264\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4043 - val_loss: 0.4254\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4262\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4283\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4036 - val_loss: 0.4258\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4039 - val_loss: 0.4270\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4032 - val_loss: 0.4269\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4030 - val_loss: 0.4273\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4394 - val_loss: 0.4210\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4256 - val_loss: 0.4175\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4235 - val_loss: 0.4166\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4221 - val_loss: 0.4165\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4211 - val_loss: 0.4159\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4201 - val_loss: 0.4161\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4192 - val_loss: 0.4155\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4187 - val_loss: 0.4157\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4180 - val_loss: 0.4161\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4175 - val_loss: 0.4160\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4169 - val_loss: 0.4162\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4164 - val_loss: 0.4157\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4162 - val_loss: 0.4154\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.4174\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4149 - val_loss: 0.4169\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.4181\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4142 - val_loss: 0.4167\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4135 - val_loss: 0.4175\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4131 - val_loss: 0.4179\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4127 - val_loss: 0.4178\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4122 - val_loss: 0.4170\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4124 - val_loss: 0.4188\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4118 - val_loss: 0.4168\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4111 - val_loss: 0.4182\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4106 - val_loss: 0.4181\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4103 - val_loss: 0.4179\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4104 - val_loss: 0.4204\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4200\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4186\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4094 - val_loss: 0.4187\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.4184\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4090 - val_loss: 0.4176\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 0.4182\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4086 - val_loss: 0.4187\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4081 - val_loss: 0.4198\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4082 - val_loss: 0.4202\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4078 - val_loss: 0.4192\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4073 - val_loss: 0.4202\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4070 - val_loss: 0.4202\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4073 - val_loss: 0.4188\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4068 - val_loss: 0.4189\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4068 - val_loss: 0.4193\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4067 - val_loss: 0.4210\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4063 - val_loss: 0.4213\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4063 - val_loss: 0.4191\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4056 - val_loss: 0.4208\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4057 - val_loss: 0.4213\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4054 - val_loss: 0.4208\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4047 - val_loss: 0.4202\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4047 - val_loss: 0.4223\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4252 - val_loss: 0.4492\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4175 - val_loss: 0.4483\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4157 - val_loss: 0.4483\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4143 - val_loss: 0.4471\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4131 - val_loss: 0.4469\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4119 - val_loss: 0.4469\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4114 - val_loss: 0.4455\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4107 - val_loss: 0.4454\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4100 - val_loss: 0.4473\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4094 - val_loss: 0.4466\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4086 - val_loss: 0.4460\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4078 - val_loss: 0.4449\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4458\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4070 - val_loss: 0.4467\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4067 - val_loss: 0.4467\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4062 - val_loss: 0.4459\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4057 - val_loss: 0.4468\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4052 - val_loss: 0.4481\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4048 - val_loss: 0.4472\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4045 - val_loss: 0.4485\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4038 - val_loss: 0.4484\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4033 - val_loss: 0.4500\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4032 - val_loss: 0.4482\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4024 - val_loss: 0.4496\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4022 - val_loss: 0.4484\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4017 - val_loss: 0.4503\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4011 - val_loss: 0.4516\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4008 - val_loss: 0.4497\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4008 - val_loss: 0.4497\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4003 - val_loss: 0.4521\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3999 - val_loss: 0.4509\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3997 - val_loss: 0.4514\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3998 - val_loss: 0.4504\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3990 - val_loss: 0.4536\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3989 - val_loss: 0.4524\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3989 - val_loss: 0.4530\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3984 - val_loss: 0.4530\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3980 - val_loss: 0.4524\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3977 - val_loss: 0.4524\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3972 - val_loss: 0.4527\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3969 - val_loss: 0.4536\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3973 - val_loss: 0.4542\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3969 - val_loss: 0.4525\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3967 - val_loss: 0.4562\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3962 - val_loss: 0.4516\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3963 - val_loss: 0.4586\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3957 - val_loss: 0.4548\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3956 - val_loss: 0.4547\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3953 - val_loss: 0.4574\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3949 - val_loss: 0.4555\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4292 - val_loss: 0.4338\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4215 - val_loss: 0.4324\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4202 - val_loss: 0.4314\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4192 - val_loss: 0.4311\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4183 - val_loss: 0.4313\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4173 - val_loss: 0.4319\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4167 - val_loss: 0.4310\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4159 - val_loss: 0.4314\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4155 - val_loss: 0.4307\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4151 - val_loss: 0.4301\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4143 - val_loss: 0.4317\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4141 - val_loss: 0.4327\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4138 - val_loss: 0.4321\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4133 - val_loss: 0.4315\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4129 - val_loss: 0.4335\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4126 - val_loss: 0.4334\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4122 - val_loss: 0.4334\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4119 - val_loss: 0.4343\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4114 - val_loss: 0.4349\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4112 - val_loss: 0.4351\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4109 - val_loss: 0.4350\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4104 - val_loss: 0.4364\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4102 - val_loss: 0.4369\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4383\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4092 - val_loss: 0.4375\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4093 - val_loss: 0.4393\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4089 - val_loss: 0.4384\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 0.4388\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4084 - val_loss: 0.4412\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4078 - val_loss: 0.4398\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4078 - val_loss: 0.4437\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4078 - val_loss: 0.4428\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4074 - val_loss: 0.4444\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4068 - val_loss: 0.4425\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4073 - val_loss: 0.4447\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4067 - val_loss: 0.4450\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4065 - val_loss: 0.4446\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4063 - val_loss: 0.4483\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4474\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4060 - val_loss: 0.4466\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4056 - val_loss: 0.4461\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4055 - val_loss: 0.4483\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4047 - val_loss: 0.4477\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4048 - val_loss: 0.4491\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4050 - val_loss: 0.4482\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4044 - val_loss: 0.4504\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4043 - val_loss: 0.4512\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4038 - val_loss: 0.4492\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4038 - val_loss: 0.4537\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4033 - val_loss: 0.4503\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4379 - val_loss: 0.4046\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4287 - val_loss: 0.4042\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4264 - val_loss: 0.4048\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4249 - val_loss: 0.4036\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4243 - val_loss: 0.4032\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4230 - val_loss: 0.4024\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4224 - val_loss: 0.4040\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4212 - val_loss: 0.4021\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4209 - val_loss: 0.4013\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4204 - val_loss: 0.4062\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4198 - val_loss: 0.4065\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4193 - val_loss: 0.4031\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4190 - val_loss: 0.4050\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4183 - val_loss: 0.4029\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4177 - val_loss: 0.4037\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4177 - val_loss: 0.4028\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4170 - val_loss: 0.4026\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4168 - val_loss: 0.4046\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4162 - val_loss: 0.4062\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4160 - val_loss: 0.4053\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4155 - val_loss: 0.4058\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4151 - val_loss: 0.4049\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4149 - val_loss: 0.4081\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4143 - val_loss: 0.4099\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4142 - val_loss: 0.4076\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4137 - val_loss: 0.4079\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4135 - val_loss: 0.4066\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4133 - val_loss: 0.4064\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4130 - val_loss: 0.4065\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4125 - val_loss: 0.4077\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4123 - val_loss: 0.4077\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4119 - val_loss: 0.4091\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4116 - val_loss: 0.4066\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4113 - val_loss: 0.4068\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4114 - val_loss: 0.4100\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4110 - val_loss: 0.4072\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4104 - val_loss: 0.4131\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4111\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4101 - val_loss: 0.4110\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4100 - val_loss: 0.4108\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4098 - val_loss: 0.4142\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4097 - val_loss: 0.4102\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4098 - val_loss: 0.4116\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.4112\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4090 - val_loss: 0.4109\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4090 - val_loss: 0.4152\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4213\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4084 - val_loss: 0.4156\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4084 - val_loss: 0.4136\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4076 - val_loss: 0.4115\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4288 - val_loss: 0.4629\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4169 - val_loss: 0.4599\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4128 - val_loss: 0.4608\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4584\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.4609\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4087 - val_loss: 0.4599\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4078 - val_loss: 0.4597\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4072 - val_loss: 0.4603\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4068 - val_loss: 0.4596\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4055 - val_loss: 0.4610\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4054 - val_loss: 0.4613\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4051 - val_loss: 0.4610\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4046 - val_loss: 0.4620\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4040 - val_loss: 0.4639\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4039 - val_loss: 0.4616\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4613\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4623\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4023 - val_loss: 0.4639\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4023 - val_loss: 0.4619\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4021 - val_loss: 0.4630\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4018 - val_loss: 0.4637\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4015 - val_loss: 0.4634\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4012 - val_loss: 0.4631\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4010 - val_loss: 0.4635\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4008 - val_loss: 0.4635\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4007 - val_loss: 0.4617\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3999 - val_loss: 0.4635\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3998 - val_loss: 0.4627\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3994 - val_loss: 0.4632\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3999 - val_loss: 0.4635\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3991 - val_loss: 0.4619\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3987 - val_loss: 0.4643\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3986 - val_loss: 0.4629\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3985 - val_loss: 0.4638\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3982 - val_loss: 0.4630\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3980 - val_loss: 0.4639\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3970 - val_loss: 0.4653\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3974 - val_loss: 0.4659\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3976 - val_loss: 0.4655\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3972 - val_loss: 0.4640\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3971 - val_loss: 0.4647\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3969 - val_loss: 0.4648\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3963 - val_loss: 0.4660\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3963 - val_loss: 0.4673\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3958 - val_loss: 0.4673\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3960 - val_loss: 0.4648\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3955 - val_loss: 0.4659\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3955 - val_loss: 0.4650\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3950 - val_loss: 0.4701\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3951 - val_loss: 0.4638\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4352 - val_loss: 0.4159\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4265 - val_loss: 0.4148\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4240 - val_loss: 0.4130\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4229 - val_loss: 0.4140\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4218 - val_loss: 0.4120\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4210 - val_loss: 0.4137\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4202 - val_loss: 0.4136\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4194 - val_loss: 0.4131\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4187 - val_loss: 0.4124\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4179 - val_loss: 0.4137\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4172 - val_loss: 0.4126\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4167 - val_loss: 0.4134\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4161 - val_loss: 0.4138\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4158 - val_loss: 0.4125\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4151 - val_loss: 0.4121\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.4148\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4142 - val_loss: 0.4150\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4136 - val_loss: 0.4154\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4134 - val_loss: 0.4162\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4124 - val_loss: 0.4145\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4129 - val_loss: 0.4159\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4120 - val_loss: 0.4156\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4118 - val_loss: 0.4168\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4112 - val_loss: 0.4161\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4109 - val_loss: 0.4161\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4104 - val_loss: 0.4175\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.4177\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.4181\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4097 - val_loss: 0.4188\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4091 - val_loss: 0.4181\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4088 - val_loss: 0.4197\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4087 - val_loss: 0.4202\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4080 - val_loss: 0.4198\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4079 - val_loss: 0.4207\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4077 - val_loss: 0.4215\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4071 - val_loss: 0.4199\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4235\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.4062 - val_loss: 0.4220\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4062 - val_loss: 0.4230\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4234\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4053 - val_loss: 0.4233\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4052 - val_loss: 0.4222\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4046 - val_loss: 0.4274\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4042 - val_loss: 0.4243\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4043 - val_loss: 0.4296\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4241\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4037 - val_loss: 0.4253\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4036 - val_loss: 0.4325\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4029 - val_loss: 0.4263\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4028 - val_loss: 0.4287\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4416 - val_loss: 0.4301\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4274 - val_loss: 0.4278\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4249 - val_loss: 0.4270\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4229 - val_loss: 0.4274\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4214 - val_loss: 0.4271\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4201 - val_loss: 0.4273\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4192 - val_loss: 0.4261\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4181 - val_loss: 0.4262\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4174 - val_loss: 0.4269\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4171 - val_loss: 0.4259\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4163 - val_loss: 0.4266\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4157 - val_loss: 0.4275\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4152 - val_loss: 0.4284\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.4289\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4139 - val_loss: 0.4286\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4132 - val_loss: 0.4291\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4129 - val_loss: 0.4294\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4122 - val_loss: 0.4297\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4118 - val_loss: 0.4324\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4113 - val_loss: 0.4296\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4108 - val_loss: 0.4296\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4102 - val_loss: 0.4303\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4098 - val_loss: 0.4304\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4089 - val_loss: 0.4348\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4086 - val_loss: 0.4304\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4082 - val_loss: 0.4312\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.4356\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4070 - val_loss: 0.4322\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4067 - val_loss: 0.4318\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4063 - val_loss: 0.4324\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4059 - val_loss: 0.4326\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4058 - val_loss: 0.4330\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4052 - val_loss: 0.4357\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4336\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4043 - val_loss: 0.4345\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4043 - val_loss: 0.4336\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4357\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4036 - val_loss: 0.4360\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4035 - val_loss: 0.4346\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4360\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4025 - val_loss: 0.4371\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4024 - val_loss: 0.4365\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4022 - val_loss: 0.4344\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4015 - val_loss: 0.4374\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4018 - val_loss: 0.4365\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4012 - val_loss: 0.4365\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4007 - val_loss: 0.4350\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4007 - val_loss: 0.4350\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4002 - val_loss: 0.4374\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4002 - val_loss: 0.4370\n",
      "469/469 [==============================] - 1s 3ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4410 - val_loss: 0.4364\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4231 - val_loss: 0.4324\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4204 - val_loss: 0.4306\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4181 - val_loss: 0.4299\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4171 - val_loss: 0.4298\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4160 - val_loss: 0.4308\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4157 - val_loss: 0.4307\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4148 - val_loss: 0.4300\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4144 - val_loss: 0.4299\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4137 - val_loss: 0.4304\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4132 - val_loss: 0.4310\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4128 - val_loss: 0.4312\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4121 - val_loss: 0.4318\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4117 - val_loss: 0.4311\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4309\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4109 - val_loss: 0.4323\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4104 - val_loss: 0.4321\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4100 - val_loss: 0.4317\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4097 - val_loss: 0.4316\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4090 - val_loss: 0.4320\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4091 - val_loss: 0.4316\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4090 - val_loss: 0.4315\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4325\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4076 - val_loss: 0.4339\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4080 - val_loss: 0.4323\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.4339\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4330\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4072 - val_loss: 0.4338\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4070 - val_loss: 0.4343\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4064 - val_loss: 0.4354\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4064 - val_loss: 0.4363\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4059 - val_loss: 0.4347\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4060 - val_loss: 0.4354\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4056 - val_loss: 0.4360\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4051 - val_loss: 0.4356\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4363\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4371\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4042 - val_loss: 0.4370\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4046 - val_loss: 0.4373\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4042 - val_loss: 0.4382\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4036 - val_loss: 0.4383\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4366\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4033 - val_loss: 0.4382\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4031 - val_loss: 0.4379\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4399\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4026 - val_loss: 0.4387\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4026 - val_loss: 0.4385\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4020 - val_loss: 0.4391\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4019 - val_loss: 0.4390\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4020 - val_loss: 0.4397\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "90\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4359 - val_loss: 0.4187\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4273 - val_loss: 0.4164\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4248 - val_loss: 0.4148\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4229 - val_loss: 0.4147\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4217 - val_loss: 0.4133\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4206 - val_loss: 0.4136\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4198 - val_loss: 0.4126\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4191 - val_loss: 0.4136\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4185 - val_loss: 0.4127\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4177 - val_loss: 0.4145\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4172 - val_loss: 0.4131\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4171 - val_loss: 0.4135\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4161 - val_loss: 0.4137\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4159 - val_loss: 0.4120\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4157 - val_loss: 0.4117\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4150 - val_loss: 0.4120\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4147 - val_loss: 0.4131\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4145 - val_loss: 0.4138\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4141 - val_loss: 0.4133\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4139 - val_loss: 0.4139\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4134 - val_loss: 0.4125\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4130 - val_loss: 0.4130\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4128 - val_loss: 0.4136\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4125 - val_loss: 0.4140\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4122 - val_loss: 0.4140\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4120 - val_loss: 0.4149\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4118 - val_loss: 0.4136\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4114 - val_loss: 0.4149\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4111 - val_loss: 0.4136\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4106 - val_loss: 0.4145\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4105 - val_loss: 0.4138\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4148\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4094 - val_loss: 0.4154\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4093 - val_loss: 0.4157\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4090 - val_loss: 0.4164\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4090 - val_loss: 0.4145\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4149\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4084 - val_loss: 0.4155\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4081 - val_loss: 0.4163\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4078 - val_loss: 0.4169\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4076 - val_loss: 0.4165\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4073 - val_loss: 0.4159\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4069 - val_loss: 0.4175\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4071 - val_loss: 0.4170\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4062 - val_loss: 0.4174\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4062 - val_loss: 0.4159\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4064 - val_loss: 0.4177\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4173\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4054 - val_loss: 0.4182\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4055 - val_loss: 0.4179\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4425 - val_loss: 0.4209\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4277 - val_loss: 0.4163\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4252 - val_loss: 0.4147\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4239 - val_loss: 0.4137\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4225 - val_loss: 0.4127\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4219 - val_loss: 0.4127\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4211 - val_loss: 0.4131\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4205 - val_loss: 0.4135\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4198 - val_loss: 0.4126\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4189 - val_loss: 0.4138\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4181 - val_loss: 0.4146\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4172 - val_loss: 0.4137\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4167 - val_loss: 0.4136\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4160 - val_loss: 0.4146\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4150 - val_loss: 0.4157\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4147 - val_loss: 0.4157\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4140 - val_loss: 0.4160\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4132 - val_loss: 0.4155\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4124 - val_loss: 0.4163\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4120 - val_loss: 0.4165\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4119 - val_loss: 0.4144\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4113 - val_loss: 0.4154\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4108 - val_loss: 0.4182\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.4185\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4190\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4094 - val_loss: 0.4177\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4089 - val_loss: 0.4195\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4090 - val_loss: 0.4205\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4081 - val_loss: 0.4193\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4078 - val_loss: 0.4210\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4076 - val_loss: 0.4191\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4070 - val_loss: 0.4195\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4071 - val_loss: 0.4198\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4064 - val_loss: 0.4210\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4062 - val_loss: 0.4226\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4203\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4055 - val_loss: 0.4214\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4056 - val_loss: 0.4212\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4051 - val_loss: 0.4209\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4047 - val_loss: 0.4227\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4043 - val_loss: 0.4229\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4044 - val_loss: 0.4224\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4222\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4036 - val_loss: 0.4218\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4031 - val_loss: 0.4232\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4032 - val_loss: 0.4218\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4026 - val_loss: 0.4245\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4024 - val_loss: 0.4217\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4023 - val_loss: 0.4246\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4023 - val_loss: 0.4224\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4438 - val_loss: 0.3859\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4337 - val_loss: 0.3844\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4317 - val_loss: 0.3852\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4304 - val_loss: 0.3871\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4293 - val_loss: 0.3843\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4282 - val_loss: 0.3846\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4275 - val_loss: 0.3843\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4267 - val_loss: 0.3840\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4260 - val_loss: 0.3843\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4256 - val_loss: 0.3856\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4252 - val_loss: 0.3844\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4244 - val_loss: 0.3853\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4237 - val_loss: 0.3850\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4233 - val_loss: 0.3840\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4228 - val_loss: 0.3851\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4220 - val_loss: 0.3846\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4218 - val_loss: 0.3844\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4210 - val_loss: 0.3859\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4211 - val_loss: 0.3851\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4205 - val_loss: 0.3848\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4197 - val_loss: 0.3868\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4193 - val_loss: 0.3865\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4191 - val_loss: 0.3860\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4186 - val_loss: 0.3872\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4179 - val_loss: 0.3882\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4172 - val_loss: 0.3860\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4171 - val_loss: 0.3860\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4165 - val_loss: 0.3889\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4166 - val_loss: 0.3872\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4158 - val_loss: 0.3889\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.3878\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.3862\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4150 - val_loss: 0.3880\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4150 - val_loss: 0.3895\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4140 - val_loss: 0.3869\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4149 - val_loss: 0.3882\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4135 - val_loss: 0.3901\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4133 - val_loss: 0.3923\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4128 - val_loss: 0.3907\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4127 - val_loss: 0.3945\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4124 - val_loss: 0.3941\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4123 - val_loss: 0.3947\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4120 - val_loss: 0.3921\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4118 - val_loss: 0.3962\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4118 - val_loss: 0.3945\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4114 - val_loss: 0.3954\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4106 - val_loss: 0.3909\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4107 - val_loss: 0.3958\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4103 - val_loss: 0.3954\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4105 - val_loss: 0.3990\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4341 - val_loss: 0.4380\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4229 - val_loss: 0.4358\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4206 - val_loss: 0.4342\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4196 - val_loss: 0.4319\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4186 - val_loss: 0.4319\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4175 - val_loss: 0.4314\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4168 - val_loss: 0.4298\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4161 - val_loss: 0.4304\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4156 - val_loss: 0.4295\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4154 - val_loss: 0.4292\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4149 - val_loss: 0.4290\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4142 - val_loss: 0.4292\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4139 - val_loss: 0.4295\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4136 - val_loss: 0.4299\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4130 - val_loss: 0.4278\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4125 - val_loss: 0.4301\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4125 - val_loss: 0.4294\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4122 - val_loss: 0.4280\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4117 - val_loss: 0.4300\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4111 - val_loss: 0.4288\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4112 - val_loss: 0.4293\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4104 - val_loss: 0.4303\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4106 - val_loss: 0.4295\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4101 - val_loss: 0.4290\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4095 - val_loss: 0.4285\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4090 - val_loss: 0.4317\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4090 - val_loss: 0.4296\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 0.4287\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4297\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4081 - val_loss: 0.4290\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4081 - val_loss: 0.4303\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4074 - val_loss: 0.4295\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4295\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4072 - val_loss: 0.4295\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4289\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4062 - val_loss: 0.4319\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4059 - val_loss: 0.4300\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4058 - val_loss: 0.4309\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4051 - val_loss: 0.4306\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4049 - val_loss: 0.4325\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4053 - val_loss: 0.4321\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4043 - val_loss: 0.4326\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4044 - val_loss: 0.4333\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4039 - val_loss: 0.4323\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4038 - val_loss: 0.4316\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4038 - val_loss: 0.4325\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4327\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4029 - val_loss: 0.4339\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4030 - val_loss: 0.4332\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4023 - val_loss: 0.4344\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 3s 4ms/step - loss: 0.4275 - val_loss: 0.4746\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4141 - val_loss: 0.4734\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4119 - val_loss: 0.4721\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4101 - val_loss: 0.4717\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4086 - val_loss: 0.4709\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4073 - val_loss: 0.4706\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4067 - val_loss: 0.4707\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4060 - val_loss: 0.4712\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4052 - val_loss: 0.4701\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4045 - val_loss: 0.4701\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4039 - val_loss: 0.4706\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4031 - val_loss: 0.4700\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4027 - val_loss: 0.4707\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4022 - val_loss: 0.4704\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4016 - val_loss: 0.4701\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4008 - val_loss: 0.4709\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4006 - val_loss: 0.4706\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4000 - val_loss: 0.4716\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3993 - val_loss: 0.4703\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3990 - val_loss: 0.4710\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3985 - val_loss: 0.4699\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3979 - val_loss: 0.4710\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3979 - val_loss: 0.4706\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3974 - val_loss: 0.4700\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3970 - val_loss: 0.4705\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3964 - val_loss: 0.4715\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3961 - val_loss: 0.4707\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3957 - val_loss: 0.4712\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3953 - val_loss: 0.4715\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3950 - val_loss: 0.4705\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3945 - val_loss: 0.4707\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3942 - val_loss: 0.4710\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3939 - val_loss: 0.4708\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3937 - val_loss: 0.4706\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3934 - val_loss: 0.4709\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3931 - val_loss: 0.4716\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3925 - val_loss: 0.4712\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3925 - val_loss: 0.4712\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3922 - val_loss: 0.4710\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3916 - val_loss: 0.4723\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3914 - val_loss: 0.4719\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3913 - val_loss: 0.4729\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3907 - val_loss: 0.4723\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3907 - val_loss: 0.4742\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3900 - val_loss: 0.4727\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3901 - val_loss: 0.4735\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3900 - val_loss: 0.4737\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3894 - val_loss: 0.4733\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3894 - val_loss: 0.4733\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3890 - val_loss: 0.4734\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4358 - val_loss: 0.4162\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4272 - val_loss: 0.4140\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4247 - val_loss: 0.4126\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4233 - val_loss: 0.4125\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4222 - val_loss: 0.4117\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4210 - val_loss: 0.4157\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4204 - val_loss: 0.4130\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4194 - val_loss: 0.4123\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4185 - val_loss: 0.4115\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4183 - val_loss: 0.4124\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4172 - val_loss: 0.4126\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4170 - val_loss: 0.4119\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4160 - val_loss: 0.4129\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4154 - val_loss: 0.4135\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4155 - val_loss: 0.4137\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.4127\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4142 - val_loss: 0.4122\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4138 - val_loss: 0.4135\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4137 - val_loss: 0.4142\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4135 - val_loss: 0.4129\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4128 - val_loss: 0.4145\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4128 - val_loss: 0.4148\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4120 - val_loss: 0.4140\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4123 - val_loss: 0.4155\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4115 - val_loss: 0.4152\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4116 - val_loss: 0.4147\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4166\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4109 - val_loss: 0.4168\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4149\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4149\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4100 - val_loss: 0.4150\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4098 - val_loss: 0.4159\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4098 - val_loss: 0.4150\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4092 - val_loss: 0.4150\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 0.4172\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4084 - val_loss: 0.4210\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4085 - val_loss: 0.4164\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4085 - val_loss: 0.4173\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4082 - val_loss: 0.4156\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4072 - val_loss: 0.4171\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4173\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4074 - val_loss: 0.4167\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4071 - val_loss: 0.4172\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4066 - val_loss: 0.4201\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4070 - val_loss: 0.4189\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4059 - val_loss: 0.4188\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4063 - val_loss: 0.4189\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4061 - val_loss: 0.4187\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4058 - val_loss: 0.4196\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4054 - val_loss: 0.4198\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4345 - val_loss: 0.4373\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4209 - val_loss: 0.4348\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4183 - val_loss: 0.4353\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4171 - val_loss: 0.4335\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4160 - val_loss: 0.4335\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4152 - val_loss: 0.4333\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4145 - val_loss: 0.4334\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4140 - val_loss: 0.4332\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4133 - val_loss: 0.4322\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4134 - val_loss: 0.4331\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4126 - val_loss: 0.4335\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4120 - val_loss: 0.4345\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4118 - val_loss: 0.4332\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4115 - val_loss: 0.4336\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4112 - val_loss: 0.4344\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4343\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4106 - val_loss: 0.4348\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4101 - val_loss: 0.4338\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4097 - val_loss: 0.4365\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4098 - val_loss: 0.4357\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4095 - val_loss: 0.4355\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4089 - val_loss: 0.4361\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4081 - val_loss: 0.4370\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.4082 - val_loss: 0.4358\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4076 - val_loss: 0.4366\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4071 - val_loss: 0.4369\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4072 - val_loss: 0.4367\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4070 - val_loss: 0.4368\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4386\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4062 - val_loss: 0.4366\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4356\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4055 - val_loss: 0.4374\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4051 - val_loss: 0.4362\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4053 - val_loss: 0.4374\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4050 - val_loss: 0.4365\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4365\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4040 - val_loss: 0.4371\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4040 - val_loss: 0.4377\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4041 - val_loss: 0.4368\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4365\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4034 - val_loss: 0.4402\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4380\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4030 - val_loss: 0.4374\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4023 - val_loss: 0.4366\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4023 - val_loss: 0.4392\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4019 - val_loss: 0.4388\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4019 - val_loss: 0.4356\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4016 - val_loss: 0.4393\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4012 - val_loss: 0.4420\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4011 - val_loss: 0.4413\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4397 - val_loss: 0.4067\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4299 - val_loss: 0.4041\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4281 - val_loss: 0.4023\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4267 - val_loss: 0.4020\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4254 - val_loss: 0.4005\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4245 - val_loss: 0.3999\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4236 - val_loss: 0.4001\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4230 - val_loss: 0.4002\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4223 - val_loss: 0.3997\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4218 - val_loss: 0.3992\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4211 - val_loss: 0.3999\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4207 - val_loss: 0.3994\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4200 - val_loss: 0.3995\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4193 - val_loss: 0.3995\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4192 - val_loss: 0.3996\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4182 - val_loss: 0.4011\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4183 - val_loss: 0.4005\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4175 - val_loss: 0.4005\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4169 - val_loss: 0.3994\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4161 - val_loss: 0.3995\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4159 - val_loss: 0.4014\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4156 - val_loss: 0.4009\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4152 - val_loss: 0.4004\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4142 - val_loss: 0.4025\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4140 - val_loss: 0.4017\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4141 - val_loss: 0.4029\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4132 - val_loss: 0.4038\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4132 - val_loss: 0.4027\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4126 - val_loss: 0.4034\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4126 - val_loss: 0.4027\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4122 - val_loss: 0.4028\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4115 - val_loss: 0.4030\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4110 - val_loss: 0.4048\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4108 - val_loss: 0.4057\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4031\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.4047\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4102 - val_loss: 0.4033\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4095 - val_loss: 0.4043\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4094 - val_loss: 0.4050\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4092 - val_loss: 0.4041\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4086 - val_loss: 0.4050\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4084 - val_loss: 0.4046\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4087 - val_loss: 0.4056\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4081 - val_loss: 0.4064\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.4061\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4069 - val_loss: 0.4064\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4072 - val_loss: 0.4064\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4067 - val_loss: 0.4072\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4068 - val_loss: 0.4059\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4064 - val_loss: 0.4057\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4358 - val_loss: 0.4179\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4279 - val_loss: 0.4158\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4253 - val_loss: 0.4141\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4237 - val_loss: 0.4145\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4224 - val_loss: 0.4144\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4213 - val_loss: 0.4134\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4208 - val_loss: 0.4141\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4201 - val_loss: 0.4137\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4192 - val_loss: 0.4151\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4191 - val_loss: 0.4141\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4184 - val_loss: 0.4147\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4178 - val_loss: 0.4147\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4173 - val_loss: 0.4143\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4168 - val_loss: 0.4146\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4160 - val_loss: 0.4149\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4158 - val_loss: 0.4139\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4158 - val_loss: 0.4143\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4150 - val_loss: 0.4144\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4146 - val_loss: 0.4141\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4142 - val_loss: 0.4154\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4138 - val_loss: 0.4151\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4135 - val_loss: 0.4145\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4129 - val_loss: 0.4146\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4125 - val_loss: 0.4160\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4124 - val_loss: 0.4153\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4121 - val_loss: 0.4144\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4114 - val_loss: 0.4159\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4111 - val_loss: 0.4162\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4105 - val_loss: 0.4158\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4104 - val_loss: 0.4158\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4099 - val_loss: 0.4155\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4093 - val_loss: 0.4169\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4095 - val_loss: 0.4165\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4086 - val_loss: 0.4189\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4088 - val_loss: 0.4175\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4087 - val_loss: 0.4164\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - val_loss: 0.4173\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4075 - val_loss: 0.4172\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4081 - val_loss: 0.4174\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4073 - val_loss: 0.4175\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4067 - val_loss: 0.4172\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4068 - val_loss: 0.4195\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4182\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4063 - val_loss: 0.4201\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4061 - val_loss: 0.4182\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.4183\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4055 - val_loss: 0.4197\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4048 - val_loss: 0.4195\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4046 - val_loss: 0.4190\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4047 - val_loss: 0.4190\n",
      "469/469 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Rerun model 100 times\n",
    "num_reruns = 100\n",
    "alphas = []\n",
    "\n",
    "np.random.seed(519)\n",
    "seeds = np.random.randint(9999, size=num_reruns)\n",
    "\n",
    "for i in range(num_reruns):\n",
    "    model_fit = fitting_returns_data(\n",
    "        'ff6_factors_19630701_20230131.csv',\n",
    "        io_day_1_lag,\n",
    "        model_feed_forward,\n",
    "        seed = seeds[i],\n",
    "        print_summary = False)[2]\n",
    "    alphas.append(model_fit.params[0])\n",
    "    if i%10 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "k4Yfo9e5Iqze",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "k4Yfo9e5Iqze",
    "outputId": "51b81cb7-4b39-4874-c11d-3a2396fe29f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest return if we ignore robustness: 0.13485969142097237\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Highest return if we ignore robustness: {max(alphas)}')\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(alphas, bins=10, alpha=0.7, color='blue')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Histogram of Feed Forward Model Alphas')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OhpnfVxELsKF",
   "metadata": {
    "id": "OhpnfVxELsKF"
   },
   "source": [
    "The highest alpha we could get if we ignore robustness is 0.135. Simply taking the highest alpha from a bunch of random trials would cause us to falsely believe our model is better than it is. This will lead to the model's average-case performance with real money on the market being far worse than our backtest expectations, which were based on a favorable random condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cReqpTcKxjl0",
   "metadata": {
    "id": "cReqpTcKxjl0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
