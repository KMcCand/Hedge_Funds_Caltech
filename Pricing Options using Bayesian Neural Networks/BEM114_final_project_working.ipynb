{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e677baf",
   "metadata": {
    "id": "9e677baf"
   },
   "source": [
    "# Pricing Options using Bayesian Neural Networks\n",
    "\n",
    "**Names:** Andrew Zabelo, Daniel Wen, Kyle McCandless  \n",
    "**Student IDs:** 2176083, 2159859, 2157818\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53cd54f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ML tools\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# BNN specific\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "import torch.nn as nn\n",
    "from pyro.infer import MCMC, NUTS\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "# We'll suppress some warnings from tensorflow, but feel free to\n",
    "# comment out this line and see some of the efficiency gains we can get!\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18326965",
   "metadata": {
    "id": "18326965"
   },
   "outputs": [],
   "source": [
    "# Note: All io_fns and model_fns start with an 'io_' and 'model_' prefix\n",
    "# respectively\n",
    "def fitting_returns_data(data_path,\n",
    "                         io_fn,\n",
    "                         model_fn,\n",
    "                         seed = None,\n",
    "                         print_summary = True):\n",
    "    \"\"\"\n",
    "    Fits data to supplied model and provides returns and alpha estimates.\n",
    "\n",
    "        Parameters:\n",
    "            data_path (str): path to factor data with first column as date\n",
    "                6 factors, and the last column as risk free rate\n",
    "            io_fn (fn): a function to calculate input/output pairs for the\n",
    "                model fit\n",
    "            model_fn (fn): a function that takes in input data, output data,\n",
    "                and training and testing values, and returns the strategy\n",
    "                for each day, and the observed return for each day\n",
    "            seed (int): if given, the seed for the model fit will be set to\n",
    "                this value\n",
    "            print_summary (bool): if True, then the model won't print a summary\n",
    "                of the OLS fit for our strategy return.\n",
    "        Returns:\n",
    "            strat_df (pd.DataFrame): The weights we would give each factor\n",
    "                on each day\n",
    "            return_vector (list): The returns from our strategy on each day\n",
    "            model_OLS (RegressionResultsWrapper): A regression calculating the\n",
    "                alpha for our strategy\n",
    "    \"\"\"\n",
    "    # if given, set the random seed\n",
    "    if seed:\n",
    "        set_seed(seed)\n",
    "\n",
    "    # Load the data into a pandas DataFrame\n",
    "    data = pd.read_csv(data_path)\n",
    "    # Drop date and risk free rate\n",
    "    data = data.iloc[:, 1:7]\n",
    "\n",
    "    # Shift the data by one time step to create input/output pairs\n",
    "    X, y = io_fn(data)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Fit the model\n",
    "    strat_df, return_vector = model_fn(X, y, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Calculate alpha\n",
    "    y_ols = sm.add_constant(y)\n",
    "    model_OLS = sm.OLS(return_vector, y_ols).fit()\n",
    "    if print_summary:\n",
    "        print(model_OLS.summary())\n",
    "\n",
    "    return strat_df, return_vector, model_OLS\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    # Adding a fixed seed from this solution: https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras\n",
    "    # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    # 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.compat.v1.set_random_seed(seed_value)\n",
    "    return\n",
    "\n",
    "def io_day_1_lag(data):\n",
    "    # Create input output pairs, where the input is the previous day of data\n",
    "    # and the output is the current day of data.\n",
    "    X = data.shift(1).dropna().reset_index(drop=True)\n",
    "    y = data.dropna().iloc[1:,:].reset_index(drop=True)\n",
    "    return X, y\n",
    "\n",
    "def io_day_5_lag(data):\n",
    "    X = data.shift(1).add_suffix('_lag1')\n",
    "    for i in range(2, 6):\n",
    "        shifted_data = data.shift(i).add_suffix('_lag{}'.format(i))\n",
    "        X = pd.concat([X, shifted_data], axis=1)\n",
    "    X = X.dropna().reset_index(drop=True)\n",
    "    y = data.iloc[5:,:].reset_index(drop=True)\n",
    "    return X, y\n",
    "\n",
    "def io_day_1_lag_second_order_input(data):\n",
    "    # Create input output pairs where input data includes second order interactions\n",
    "    X, y = io_day_1_lag(data)\n",
    "    cols = X.columns\n",
    "\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i, len(cols)):\n",
    "            col_name = cols[i] + cols[j]\n",
    "            col_values = X[cols[i]] * X[cols[j]]\n",
    "            X[col_name] = col_values\n",
    "    return X, y\n",
    "\n",
    "def model_feed_forward(X, y, X_train, y_train, X_val, y_val):\n",
    "    # Define the neural network model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(6, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Train the model, verbose = 0 means reports aren't printed\n",
    "    # at the end of each epoch\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=50,\n",
    "              validation_data=(X_val, y_val))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "\n",
    "    return predictions_to_returns(pred_df, y)\n",
    "\n",
    "def predictions_to_returns(pred_df, y):\n",
    "    # Given the predictions of each factor for each day, calculate our\n",
    "    # strategy for each day, and the returns for each day\n",
    "\n",
    "    # Apply our strategy to our predictions\n",
    "    strat_df = pred_df.apply(lambda row : max_predicted_factor_strat(row), axis = 1)\n",
    "\n",
    "    # Calculate our returns\n",
    "    return_vector = np.multiply(strat_df,np.asarray(y)).apply(sum, axis = 1)\n",
    "\n",
    "    return strat_df, return_vector\n",
    "\n",
    "def predictions_to_returns_max_sharpe(pred_df, std_df, y):\n",
    "    # Given the predictions of each factor for each day, calculate our\n",
    "    # strategy for each day, and the returns for each day\n",
    "\n",
    "    # Apply our strategy to our predictions\n",
    "    strat_df = pred_df.apply(lambda row : max_predicted_factor_strat(row), axis = 1)\n",
    "\n",
    "    # Calculate our returns\n",
    "    return_vector = np.multiply(strat_df,np.asarray(y)).apply(sum, axis = 1)\n",
    "\n",
    "    return strat_df, return_vector\n",
    "\n",
    "def max_predicted_factor_strat(row):\n",
    "    # For each day, set our strategy to be the factor with\n",
    "    # the highest predicted return\n",
    "    max_pred_return = max(row)\n",
    "    row_list = [x == max_pred_return for x in row]\n",
    "    return pd.Series(row_list)\n",
    "\n",
    "def max_sharpe_ratio_factor_strat(row):\n",
    "    # For each day, set our strategy to be the factor with\n",
    "    # the highest predicted sharpe ratio\n",
    "    max_pred_return = max(row)\n",
    "    row_list = [x == max_pred_return for x in row]\n",
    "    return pd.Series(row_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2715a",
   "metadata": {},
   "source": [
    "## Predicting Factor returns using a Bayesian Neural Network\n",
    "\n",
    "### Define the BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d7b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstBNN(PyroModule):\n",
    "    def __init__(self, in_dim=6, out_dim=6, hid_dim=10, prior_scale=1.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = nn.Tanh()  # or nn.ReLU()\n",
    "        self.layer1 = PyroModule[nn.Linear](in_dim, hid_dim)  # Input to hidden layer\n",
    "        self.layer2 = PyroModule[nn.Linear](hid_dim, out_dim)  # Hidden to output layer\n",
    "\n",
    "        # Set layer parameters as random variables\n",
    "        self.layer1.weight = PyroSample(dist.Normal(0., prior_scale).expand([hid_dim, in_dim]).to_event(2))\n",
    "        self.layer1.bias = PyroSample(dist.Normal(0., prior_scale).expand([hid_dim]).to_event(1))\n",
    "        self.layer2.weight = PyroSample(dist.Normal(0., prior_scale).expand([out_dim, hid_dim]).to_event(2))\n",
    "        self.layer2.bias = PyroSample(dist.Normal(0., prior_scale).expand([out_dim]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        mu = self.layer2(x).squeeze()\n",
    "        sigma = pyro.sample(\"sigma\", dist.Gamma(0.5, 2.0))  # Infer the response noise\n",
    "\n",
    "        # Sampling model\n",
    "        with pyro.plate(\"data\", 6):\n",
    "            obs = pyro.sample(\"obs\", dist.Normal(mu, sigma*sigma), obs=y)\n",
    "        return mu\n",
    "    \n",
    "# Calculate returns and alphas using BNN with 5 day lag\n",
    "def model_bnn(X, y, X_train, y_train, X_val, y_val):\n",
    "    # Convert data\n",
    "    x_train_tensor = torch.tensor(X_train.values).float()\n",
    "    y_train_tensor = torch.tensor(y_train.values).float()\n",
    "    x_val_tensor = torch.tensor(X_val.values).float()\n",
    "    x_val_tensor = torch.tensor(y_val.values).float()\n",
    "    x_tensor = torch.tensor(X.values).float()\n",
    "    y_tensor = torch.tensor(y.values).float()\n",
    "\n",
    "    # Define the neural network model\n",
    "    model = MyFirstBNN()\n",
    "\n",
    "    # Set Pyro random seed\n",
    "    pyro.set_rng_seed(COMMON_SEED)\n",
    "\n",
    "    # Define Hamiltonian Monte Carlo (HMC) kernel\n",
    "    # NUTS = \"No-U-Turn Sampler\" (https://arxiv.org/abs/1111.4246), gives HMC an adaptive step size\n",
    "    nuts_kernel = NUTS(model, jit_compile=True) # jit_compile=True is faster but requires PyTorch 1.6+\n",
    "\n",
    "    # Define MCMC sampler, get 50 posterior samples\n",
    "    mcmc = MCMC(nuts_kernel, num_samples=15)\n",
    "\n",
    "    # Run MCMC\n",
    "    mcmc.run(x_train_tensor, y_train_tensor)\n",
    "\n",
    "    # Make predictions\n",
    "    predictive = Predictive(model=model, posterior_samples=mcmc.get_samples())\n",
    "    preds = predictive(x_tensor)\n",
    "    \n",
    "    pred_means = preds['obs'].T.detach().numpy().mean(axis=2)\n",
    "    pred_df = pd.DataFrame(pred_means.T)\n",
    "    \n",
    "    pred_stds = preds['obs'].T.detach().numpy().std(axis=2)\n",
    "    std_df = pd.DataFrame(pred_stds.T)\n",
    "\n",
    "    return predictions_to_returns(pred_df, y)\n",
    "#     return predictions_to_returns_max_sharpe(pred_df, std_df, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e201a0d",
   "metadata": {},
   "source": [
    "### Train the BNN and calculate alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09117730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|████| 30/30 [03:56,  7.87s/it, step size=9.78e-04, acc. prob=0.977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.144\n",
      "Model:                            OLS   Adj. R-squared:                  0.144\n",
      "Method:                 Least Squares   F-statistic:                     421.6\n",
      "Date:                Thu, 06 Jun 2024   Prob (F-statistic):               0.00\n",
      "Time:                        00:46:40   Log-Likelihood:                -15192.\n",
      "No. Observations:               14998   AIC:                         3.040e+04\n",
      "Df Residuals:                   14991   BIC:                         3.045e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0456      0.005      8.349      0.000       0.035       0.056\n",
      "Mkt-RF         0.2349      0.006     40.138      0.000       0.223       0.246\n",
      "SMB            0.0918      0.011      8.616      0.000       0.071       0.113\n",
      "HML            0.2785      0.012     22.704      0.000       0.254       0.303\n",
      "RMW            0.2118      0.015     14.425      0.000       0.183       0.241\n",
      "CMA            0.1451      0.019      7.696      0.000       0.108       0.182\n",
      "MOM            0.1557      0.008     20.406      0.000       0.141       0.171\n",
      "==============================================================================\n",
      "Omnibus:                     5047.428   Durbin-Watson:                   2.003\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           694717.584\n",
      "Skew:                          -0.538   Prob(JB):                         0.00\n",
      "Kurtosis:                      36.325   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           0      1      2      3      4      5\n",
       " 0      False  False  False  False  False   True\n",
       " 1      False  False  False  False  False   True\n",
       " 2      False   True  False  False  False  False\n",
       " 3      False   True  False  False  False  False\n",
       " 4      False  False   True  False  False  False\n",
       " ...      ...    ...    ...    ...    ...    ...\n",
       " 14993  False  False  False  False  False   True\n",
       " 14994  False   True  False  False  False  False\n",
       " 14995  False  False  False   True  False  False\n",
       " 14996  False  False   True  False  False  False\n",
       " 14997  False  False   True  False  False  False\n",
       " \n",
       " [14998 rows x 6 columns],\n",
       " 0        0.42\n",
       " 1        0.41\n",
       " 2        0.09\n",
       " 3        0.07\n",
       " 4        0.09\n",
       "          ... \n",
       " 14993    0.14\n",
       " 14994   -0.62\n",
       " 14995   -0.61\n",
       " 14996    0.72\n",
       " 14997   -0.06\n",
       " Length: 14998, dtype: float64,\n",
       " <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x29e9580d0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMMON_SEED = 1\n",
    "\n",
    "fitting_returns_data(\n",
    "    'ff6_factors_19630701_20230131.csv',\n",
    "    io_day_1_lag,\n",
    "    model_bnn,\n",
    "    seed = COMMON_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cReqpTcKxjl0",
   "metadata": {
    "id": "cReqpTcKxjl0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
